{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from preprocessing_data import preprocessing_data, split_dataset, get_average\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"./data/melb_data.csv\"\n",
    "EXCLUDE_COLUMNS = ['Address','Method','SellerG','Date','Postcode','CouncilArea','Lattitude','Longtitude']\n",
    "TARGET_COLUMN_NAME = 'Price'\n",
    "SHAPE = 337\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocessing_data(DATASET_PATH,EXCLUDE_COLUMNS,TARGET_COLUMN_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = split_dataset(data, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP , `Adam`, Learning rate `0.001`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    return tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Dense(SHAPE),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    tf.keras.layers.Dense(50),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    tf.keras.layers.Dense(20),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dropout(0.5) ,\n",
    "    tf.keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "                learning_rate=0.001,\n",
    "                ),\n",
    "    loss='MSLE',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Layer dense_40 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "  2/679 [..............................] - ETA: 9:02 - loss: 182.2298WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0050s vs `on_train_batch_end` time: 1.5945s). Check your callbacks.\n",
      "677/679 [============================>.] - ETA: 0s - loss: 151.3722\n",
      "Epoch 00001: val_loss improved from inf to 128.00412, saving model to models/mlp-adam-lr-001/training__01__151.313904\\cp.ckpt\n",
      "679/679 [==============================] - 5s 7ms/step - loss: 151.3139 - val_loss: 128.0041\n",
      "Epoch 2/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 118.2115\n",
      "Epoch 00002: val_loss improved from 128.00412 to 108.39594, saving model to models/mlp-adam-lr-001/training__02__118.060799\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 118.0608 - val_loss: 108.3959\n",
      "Epoch 3/100\n",
      "676/679 [============================>.] - ETA: 0s - loss: 103.2150\n",
      "Epoch 00003: val_loss improved from 108.39594 to 96.61241, saving model to models/mlp-adam-lr-001/training__03__103.192436\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 103.1924 - val_loss: 96.6124\n",
      "Epoch 4/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 93.4818\n",
      "Epoch 00004: val_loss improved from 96.61241 to 88.51657, saving model to models/mlp-adam-lr-001/training__04__93.455605\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 93.4556 - val_loss: 88.5166\n",
      "Epoch 5/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 86.1669\n",
      "Epoch 00005: val_loss improved from 88.51657 to 81.82534, saving model to models/mlp-adam-lr-001/training__05__86.166931\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 86.1669 - val_loss: 81.8253\n",
      "Epoch 6/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 80.1351\n",
      "Epoch 00006: val_loss improved from 81.82534 to 76.31822, saving model to models/mlp-adam-lr-001/training__06__80.083366\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 80.0834 - val_loss: 76.3182\n",
      "Epoch 7/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 74.8480\n",
      "Epoch 00007: val_loss improved from 76.31822 to 71.30510, saving model to models/mlp-adam-lr-001/training__07__74.807747\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 74.8077 - val_loss: 71.3051\n",
      "Epoch 8/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 70.1779\n",
      "Epoch 00008: val_loss improved from 71.30510 to 66.93494, saving model to models/mlp-adam-lr-001/training__08__70.177872\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 70.1779 - val_loss: 66.9349\n",
      "Epoch 9/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 66.1125\n",
      "Epoch 00009: val_loss improved from 66.93494 to 63.09523, saving model to models/mlp-adam-lr-001/training__09__66.109077\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 66.1091 - val_loss: 63.0952\n",
      "Epoch 10/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 62.5117\n",
      "Epoch 00010: val_loss improved from 63.09523 to 59.54162, saving model to models/mlp-adam-lr-001/training__10__62.487377\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 62.4874 - val_loss: 59.5416\n",
      "Epoch 11/100\n",
      "666/679 [============================>.] - ETA: 0s - loss: 59.0998\n",
      "Epoch 00011: val_loss improved from 59.54162 to 56.43761, saving model to models/mlp-adam-lr-001/training__11__59.064308\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 59.0643 - val_loss: 56.4376\n",
      "Epoch 12/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 55.9971\n",
      "Epoch 00012: val_loss improved from 56.43761 to 53.55740, saving model to models/mlp-adam-lr-001/training__12__55.977638\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 55.9776 - val_loss: 53.5574\n",
      "Epoch 13/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 53.2588\n",
      "Epoch 00013: val_loss improved from 53.55740 to 50.77625, saving model to models/mlp-adam-lr-001/training__13__53.224705\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 53.2247 - val_loss: 50.7762\n",
      "Epoch 14/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 50.6835\n",
      "Epoch 00014: val_loss improved from 50.77625 to 48.41336, saving model to models/mlp-adam-lr-001/training__14__50.660053\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 50.6601 - val_loss: 48.4134\n",
      "Epoch 15/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 48.2877\n",
      "Epoch 00015: val_loss improved from 48.41336 to 46.10830, saving model to models/mlp-adam-lr-001/training__15__48.284389\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 48.2844 - val_loss: 46.1083\n",
      "Epoch 16/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 46.1366\n",
      "Epoch 00016: val_loss improved from 46.10830 to 44.15861, saving model to models/mlp-adam-lr-001/training__16__46.134739\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 46.1347 - val_loss: 44.1586\n",
      "Epoch 17/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 44.2539\n",
      "Epoch 00017: val_loss improved from 44.15861 to 42.30178, saving model to models/mlp-adam-lr-001/training__17__44.252674\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 44.2527 - val_loss: 42.3018\n",
      "Epoch 18/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 42.3277\n",
      "Epoch 00018: val_loss improved from 42.30178 to 40.56915, saving model to models/mlp-adam-lr-001/training__18__42.326298\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 42.3263 - val_loss: 40.5691\n",
      "Epoch 19/100\n",
      "676/679 [============================>.] - ETA: 0s - loss: 40.6977\n",
      "Epoch 00019: val_loss improved from 40.56915 to 39.02331, saving model to models/mlp-adam-lr-001/training__19__40.693188\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 40.6932 - val_loss: 39.0233\n",
      "Epoch 20/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 39.1231\n",
      "Epoch 00020: val_loss improved from 39.02331 to 37.44830, saving model to models/mlp-adam-lr-001/training__20__39.123123\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 39.1231 - val_loss: 37.4483\n",
      "Epoch 21/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 37.6430\n",
      "Epoch 00021: val_loss improved from 37.44830 to 36.08920, saving model to models/mlp-adam-lr-001/training__21__37.649700\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 37.6497 - val_loss: 36.0892\n",
      "Epoch 22/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 36.2768\n",
      "Epoch 00022: val_loss improved from 36.08920 to 34.80024, saving model to models/mlp-adam-lr-001/training__22__36.274246\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 36.2742 - val_loss: 34.8002\n",
      "Epoch 23/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 35.0299\n",
      "Epoch 00023: val_loss improved from 34.80024 to 33.59303, saving model to models/mlp-adam-lr-001/training__23__35.029942\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 35.0299 - val_loss: 33.5930\n",
      "Epoch 24/100\n",
      "671/679 [============================>.] - ETA: 0s - loss: 33.8166\n",
      "Epoch 00024: val_loss improved from 33.59303 to 32.42334, saving model to models/mlp-adam-lr-001/training__24__33.814709\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 33.8147 - val_loss: 32.4233\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "669/679 [============================>.] - ETA: 0s - loss: 32.6897\n",
      "Epoch 00025: val_loss improved from 32.42334 to 31.34919, saving model to models/mlp-adam-lr-001/training__25__32.676205\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 32.6762 - val_loss: 31.3492\n",
      "Epoch 26/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 31.6502\n",
      "Epoch 00026: val_loss improved from 31.34919 to 30.28727, saving model to models/mlp-adam-lr-001/training__26__31.646168\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 31.6462 - val_loss: 30.2873\n",
      "Epoch 27/100\n",
      "664/679 [============================>.] - ETA: 0s - loss: 30.6525\n",
      "Epoch 00027: val_loss improved from 30.28727 to 29.39356, saving model to models/mlp-adam-lr-001/training__27__30.638447\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 30.6384 - val_loss: 29.3936\n",
      "Epoch 28/100\n",
      "671/679 [============================>.] - ETA: 0s - loss: 29.7019\n",
      "Epoch 00028: val_loss improved from 29.39356 to 28.48583, saving model to models/mlp-adam-lr-001/training__28__29.699694\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 29.6997 - val_loss: 28.4858\n",
      "Epoch 29/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 28.8237\n",
      "Epoch 00029: val_loss improved from 28.48583 to 27.63568, saving model to models/mlp-adam-lr-001/training__29__28.813026\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 28.8130 - val_loss: 27.6357\n",
      "Epoch 30/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 27.9738\n",
      "Epoch 00030: val_loss improved from 27.63568 to 26.87317, saving model to models/mlp-adam-lr-001/training__30__27.961147\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 27.9611 - val_loss: 26.8732\n",
      "Epoch 31/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 27.1609\n",
      "Epoch 00031: val_loss improved from 26.87317 to 26.05442, saving model to models/mlp-adam-lr-001/training__31__27.159561\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 27.1596 - val_loss: 26.0544\n",
      "Epoch 32/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 26.3787\n",
      "Epoch 00032: val_loss improved from 26.05442 to 25.24768, saving model to models/mlp-adam-lr-001/training__32__26.378658\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 26.3787 - val_loss: 25.2477\n",
      "Epoch 33/100\n",
      "666/679 [============================>.] - ETA: 0s - loss: 25.6314\n",
      "Epoch 00033: val_loss improved from 25.24768 to 24.55950, saving model to models/mlp-adam-lr-001/training__33__25.625879\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 25.6259 - val_loss: 24.5595\n",
      "Epoch 34/100\n",
      "676/679 [============================>.] - ETA: 0s - loss: 24.8902\n",
      "Epoch 00034: val_loss improved from 24.55950 to 23.90801, saving model to models/mlp-adam-lr-001/training__34__24.888733\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 24.8887 - val_loss: 23.9080\n",
      "Epoch 35/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 24.2271\n",
      "Epoch 00035: val_loss improved from 23.90801 to 23.25541, saving model to models/mlp-adam-lr-001/training__35__24.221203\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 24.2212 - val_loss: 23.2554\n",
      "Epoch 36/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 23.6010\n",
      "Epoch 00036: val_loss improved from 23.25541 to 22.64714, saving model to models/mlp-adam-lr-001/training__36__23.595800\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 23.5958 - val_loss: 22.6471\n",
      "Epoch 37/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 23.0056\n",
      "Epoch 00037: val_loss improved from 22.64714 to 22.04564, saving model to models/mlp-adam-lr-001/training__37__23.006147\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 23.0061 - val_loss: 22.0456\n",
      "Epoch 38/100\n",
      "666/679 [============================>.] - ETA: 0s - loss: 22.3919\n",
      "Epoch 00038: val_loss improved from 22.04564 to 21.46056, saving model to models/mlp-adam-lr-001/training__38__22.386288\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 22.3863 - val_loss: 21.4606\n",
      "Epoch 39/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 21.8266\n",
      "Epoch 00039: val_loss improved from 21.46056 to 20.90686, saving model to models/mlp-adam-lr-001/training__39__21.830357\\cp.ckpt\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 21.8304 - val_loss: 20.9069\n",
      "Epoch 40/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 21.2062\n",
      "Epoch 00040: val_loss improved from 20.90686 to 20.33733, saving model to models/mlp-adam-lr-001/training__40__21.198120\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 21.1981 - val_loss: 20.3373\n",
      "Epoch 41/100\n",
      "661/679 [============================>.] - ETA: 0s - loss: 20.8047\n",
      "Epoch 00041: val_loss improved from 20.33733 to 19.89095, saving model to models/mlp-adam-lr-001/training__41__20.802490\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 20.8025 - val_loss: 19.8910\n",
      "Epoch 42/100\n",
      "662/679 [============================>.] - ETA: 0s - loss: 20.2853\n",
      "Epoch 00042: val_loss improved from 19.89095 to 19.39713, saving model to models/mlp-adam-lr-001/training__42__20.283762\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 20.2838 - val_loss: 19.3971\n",
      "Epoch 43/100\n",
      "664/679 [============================>.] - ETA: 0s - loss: 19.7865\n",
      "Epoch 00043: val_loss improved from 19.39713 to 18.95295, saving model to models/mlp-adam-lr-001/training__43__19.775419\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 19.7754 - val_loss: 18.9529\n",
      "Epoch 44/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 19.3325\n",
      "Epoch 00044: val_loss improved from 18.95295 to 18.55525, saving model to models/mlp-adam-lr-001/training__44__19.329426\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 19.3294 - val_loss: 18.5552\n",
      "Epoch 45/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 18.8436\n",
      "Epoch 00045: val_loss improved from 18.55525 to 18.07278, saving model to models/mlp-adam-lr-001/training__45__18.838871\\cp.ckpt\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 18.8389 - val_loss: 18.0728\n",
      "Epoch 46/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 18.4063\n",
      "Epoch 00046: val_loss improved from 18.07278 to 17.66171, saving model to models/mlp-adam-lr-001/training__46__18.403898\\cp.ckpt\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 18.4039 - val_loss: 17.6617\n",
      "Epoch 47/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 17.9893\n",
      "Epoch 00047: val_loss improved from 17.66171 to 17.22140, saving model to models/mlp-adam-lr-001/training__47__17.985956\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 17.9860 - val_loss: 17.2214\n",
      "Epoch 48/100\n",
      "664/679 [============================>.] - ETA: 0s - loss: 17.5895\n",
      "Epoch 00048: val_loss improved from 17.22140 to 16.86283, saving model to models/mlp-adam-lr-001/training__48__17.584257\\cp.ckpt\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 17.5843 - val_loss: 16.8628\n",
      "Epoch 49/100\n",
      "664/679 [============================>.] - ETA: 0s - loss: 17.2242\n",
      "Epoch 00049: val_loss improved from 16.86283 to 16.45920, saving model to models/mlp-adam-lr-001/training__49__17.218508\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 17.2185 - val_loss: 16.4592\n",
      "Epoch 50/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 16.8578\n",
      "Epoch 00050: val_loss improved from 16.45920 to 16.07661, saving model to models/mlp-adam-lr-001/training__50__16.859442\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 16.8594 - val_loss: 16.0766\n",
      "Epoch 51/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 16.5380 ETA: 0s - loss: 16.54\n",
      "Epoch 00051: val_loss improved from 16.07661 to 15.77590, saving model to models/mlp-adam-lr-001/training__51__16.533026\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 16.5330 - val_loss: 15.7759\n",
      "Epoch 52/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 16.1066\n",
      "Epoch 00052: val_loss improved from 15.77590 to 15.41984, saving model to models/mlp-adam-lr-001/training__52__16.107740\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 16.1077 - val_loss: 15.4198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 15.7937\n",
      "Epoch 00053: val_loss improved from 15.41984 to 15.10940, saving model to models/mlp-adam-lr-001/training__53__15.791765\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 15.7918 - val_loss: 15.1094\n",
      "Epoch 54/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 15.4394\n",
      "Epoch 00054: val_loss improved from 15.10940 to 14.79407, saving model to models/mlp-adam-lr-001/training__54__15.432742\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 15.4327 - val_loss: 14.7941\n",
      "Epoch 55/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 15.0976\n",
      "Epoch 00055: val_loss improved from 14.79407 to 14.46096, saving model to models/mlp-adam-lr-001/training__55__15.095289\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 15.0953 - val_loss: 14.4610\n",
      "Epoch 56/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 14.8489\n",
      "Epoch 00056: val_loss improved from 14.46096 to 14.17824, saving model to models/mlp-adam-lr-001/training__56__14.844003\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 14.8440 - val_loss: 14.1782\n",
      "Epoch 57/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 14.6021\n",
      "Epoch 00057: val_loss improved from 14.17824 to 13.85685, saving model to models/mlp-adam-lr-001/training__57__14.597569\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 14.5976 - val_loss: 13.8568\n",
      "Epoch 58/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 14.1914\n",
      "Epoch 00058: val_loss improved from 13.85685 to 13.58508, saving model to models/mlp-adam-lr-001/training__58__14.189895\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 14.1899 - val_loss: 13.5851\n",
      "Epoch 59/100\n",
      "674/679 [============================>.] - ETA: 0s - loss: 13.9234\n",
      "Epoch 00059: val_loss improved from 13.58508 to 13.32494, saving model to models/mlp-adam-lr-001/training__59__13.923973\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 13.9240 - val_loss: 13.3249\n",
      "Epoch 60/100\n",
      "676/679 [============================>.] - ETA: 0s - loss: 13.6400\n",
      "Epoch 00060: val_loss improved from 13.32494 to 13.02039, saving model to models/mlp-adam-lr-001/training__60__13.638933\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 13.6389 - val_loss: 13.0204\n",
      "Epoch 61/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 13.4130\n",
      "Epoch 00061: val_loss improved from 13.02039 to 12.78543, saving model to models/mlp-adam-lr-001/training__61__13.409012\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 13.4090 - val_loss: 12.7854\n",
      "Epoch 62/100\n",
      "674/679 [============================>.] - ETA: 0s - loss: 13.1355\n",
      "Epoch 00062: val_loss improved from 12.78543 to 12.48759, saving model to models/mlp-adam-lr-001/training__62__13.134630\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 13.1346 - val_loss: 12.4876\n",
      "Epoch 63/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 12.8412\n",
      "Epoch 00063: val_loss improved from 12.48759 to 12.27050, saving model to models/mlp-adam-lr-001/training__63__12.833373\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 12.8334 - val_loss: 12.2705\n",
      "Epoch 64/100\n",
      "671/679 [============================>.] - ETA: 0s - loss: 12.6774\n",
      "Epoch 00064: val_loss improved from 12.27050 to 12.00970, saving model to models/mlp-adam-lr-001/training__64__12.677191\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 12.6772 - val_loss: 12.0097\n",
      "Epoch 65/100\n",
      "666/679 [============================>.] - ETA: 0s - loss: 12.3920\n",
      "Epoch 00065: val_loss improved from 12.00970 to 11.80466, saving model to models/mlp-adam-lr-001/training__65__12.391984\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 12.3920 - val_loss: 11.8047\n",
      "Epoch 66/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 12.1724\n",
      "Epoch 00066: val_loss improved from 11.80466 to 11.56415, saving model to models/mlp-adam-lr-001/training__66__12.172367\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 12.1724 - val_loss: 11.5641\n",
      "Epoch 67/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 11.9341\n",
      "Epoch 00067: val_loss improved from 11.56415 to 11.37237, saving model to models/mlp-adam-lr-001/training__67__11.930873\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 11.9309 - val_loss: 11.3724\n",
      "Epoch 68/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 11.6905\n",
      "Epoch 00068: val_loss improved from 11.37237 to 11.14193, saving model to models/mlp-adam-lr-001/training__68__11.684134\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 11.6841 - val_loss: 11.1419\n",
      "Epoch 69/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 11.5264\n",
      "Epoch 00069: val_loss improved from 11.14193 to 10.92583, saving model to models/mlp-adam-lr-001/training__69__11.526688\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 11.5267 - val_loss: 10.9258\n",
      "Epoch 70/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 11.3128\n",
      "Epoch 00070: val_loss improved from 10.92583 to 10.71553, saving model to models/mlp-adam-lr-001/training__70__11.311552\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 11.3116 - val_loss: 10.7155\n",
      "Epoch 71/100\n",
      "671/679 [============================>.] - ETA: 0s - loss: 11.0919\n",
      "Epoch 00071: val_loss improved from 10.71553 to 10.54296, saving model to models/mlp-adam-lr-001/training__71__11.090867\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 11.0909 - val_loss: 10.5430\n",
      "Epoch 72/100\n",
      "671/679 [============================>.] - ETA: 0s - loss: 10.8727\n",
      "Epoch 00072: val_loss improved from 10.54296 to 10.33912, saving model to models/mlp-adam-lr-001/training__72__10.872092\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.8721 - val_loss: 10.3391\n",
      "Epoch 73/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 10.6690\n",
      "Epoch 00073: val_loss improved from 10.33912 to 10.13650, saving model to models/mlp-adam-lr-001/training__73__10.668953\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.6690 - val_loss: 10.1365\n",
      "Epoch 74/100\n",
      "660/679 [============================>.] - ETA: 0s - loss: 10.4655\n",
      "Epoch 00074: val_loss improved from 10.13650 to 9.94734, saving model to models/mlp-adam-lr-001/training__74__10.461370\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.4614 - val_loss: 9.9473\n",
      "Epoch 75/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 10.3186\n",
      "Epoch 00075: val_loss improved from 9.94734 to 9.76741, saving model to models/mlp-adam-lr-001/training__75__10.319815\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.3198 - val_loss: 9.7674\n",
      "Epoch 76/100\n",
      "671/679 [============================>.] - ETA: 0s - loss: 10.1369\n",
      "Epoch 00076: val_loss improved from 9.76741 to 9.60363, saving model to models/mlp-adam-lr-001/training__76__10.136096\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.1361 - val_loss: 9.6036\n",
      "Epoch 77/100\n",
      "671/679 [============================>.] - ETA: 0s - loss: 9.9049\n",
      "Epoch 00077: val_loss improved from 9.60363 to 9.42956, saving model to models/mlp-adam-lr-001/training__77__9.908116\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.9081 - val_loss: 9.4296\n",
      "Epoch 78/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 9.7957\n",
      "Epoch 00078: val_loss improved from 9.42956 to 9.25640, saving model to models/mlp-adam-lr-001/training__78__9.789186\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.7892 - val_loss: 9.2564\n",
      "Epoch 79/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 9.6141\n",
      "Epoch 00079: val_loss improved from 9.25640 to 9.09050, saving model to models/mlp-adam-lr-001/training__79__9.613616\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.6136 - val_loss: 9.0905\n",
      "Epoch 80/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 9.4400\n",
      "Epoch 00080: val_loss improved from 9.09050 to 8.91952, saving model to models/mlp-adam-lr-001/training__80__9.443164\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.4432 - val_loss: 8.9195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 9.2673\n",
      "Epoch 00081: val_loss improved from 8.91952 to 8.77518, saving model to models/mlp-adam-lr-001/training__81__9.263116\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.2631 - val_loss: 8.7752\n",
      "Epoch 82/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 9.1348\n",
      "Epoch 00082: val_loss improved from 8.77518 to 8.60818, saving model to models/mlp-adam-lr-001/training__82__9.133359\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.1334 - val_loss: 8.6082\n",
      "Epoch 83/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 8.9527\n",
      "Epoch 00083: val_loss improved from 8.60818 to 8.47720, saving model to models/mlp-adam-lr-001/training__83__8.958427\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 8.9584 - val_loss: 8.4772\n",
      "Epoch 84/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 8.7909\n",
      "Epoch 00084: val_loss improved from 8.47720 to 8.32964, saving model to models/mlp-adam-lr-001/training__84__8.788783\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 8.7888 - val_loss: 8.3296\n",
      "Epoch 85/100\n",
      "661/679 [============================>.] - ETA: 0s - loss: 8.6394\n",
      "Epoch 00085: val_loss improved from 8.32964 to 8.17919, saving model to models/mlp-adam-lr-001/training__85__8.640928\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 8.6409 - val_loss: 8.1792\n",
      "Epoch 86/100\n",
      "674/679 [============================>.] - ETA: 0s - loss: 8.4920\n",
      "Epoch 00086: val_loss improved from 8.17919 to 8.01892, saving model to models/mlp-adam-lr-001/training__86__8.493224\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 8.4932 - val_loss: 8.0189\n",
      "Epoch 87/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 8.3405\n",
      "Epoch 00087: val_loss improved from 8.01892 to 7.89308, saving model to models/mlp-adam-lr-001/training__87__8.336672\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 8.3367 - val_loss: 7.8931\n",
      "Epoch 88/100\n",
      "676/679 [============================>.] - ETA: 0s - loss: 8.2126\n",
      "Epoch 00088: val_loss improved from 7.89308 to 7.76452, saving model to models/mlp-adam-lr-001/training__88__8.212539\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 8.2125 - val_loss: 7.7645\n",
      "Epoch 89/100\n",
      "671/679 [============================>.] - ETA: 0s - loss: 8.0427\n",
      "Epoch 00089: val_loss improved from 7.76452 to 7.61770, saving model to models/mlp-adam-lr-001/training__89__8.040962\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 8.0410 - val_loss: 7.6177\n",
      "Epoch 90/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 7.9115\n",
      "Epoch 00090: val_loss improved from 7.61770 to 7.50326, saving model to models/mlp-adam-lr-001/training__90__7.910114\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 7.9101 - val_loss: 7.5033\n",
      "Epoch 91/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 7.8107\n",
      "Epoch 00091: val_loss improved from 7.50326 to 7.36738, saving model to models/mlp-adam-lr-001/training__91__7.808813\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 7.8088 - val_loss: 7.3674\n",
      "Epoch 92/100\n",
      "660/679 [============================>.] - ETA: 0s - loss: 7.6567\n",
      "Epoch 00092: val_loss improved from 7.36738 to 7.24318, saving model to models/mlp-adam-lr-001/training__92__7.655265\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 7.6553 - val_loss: 7.2432\n",
      "Epoch 93/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 7.5539\n",
      "Epoch 00093: val_loss improved from 7.24318 to 7.11406, saving model to models/mlp-adam-lr-001/training__93__7.555306\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 7.5553 - val_loss: 7.1141\n",
      "Epoch 94/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 7.4265\n",
      "Epoch 00094: val_loss improved from 7.11406 to 6.97557, saving model to models/mlp-adam-lr-001/training__94__7.423763\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 7.4238 - val_loss: 6.9756\n",
      "Epoch 95/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 7.2996\n",
      "Epoch 00095: val_loss improved from 6.97557 to 6.87225, saving model to models/mlp-adam-lr-001/training__95__7.300062\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 7.3001 - val_loss: 6.8722\n",
      "Epoch 96/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 7.1637\n",
      "Epoch 00096: val_loss improved from 6.87225 to 6.74360, saving model to models/mlp-adam-lr-001/training__96__7.164107\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 7.1641 - val_loss: 6.7436\n",
      "Epoch 97/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 7.0717\n",
      "Epoch 00097: val_loss improved from 6.74360 to 6.65984, saving model to models/mlp-adam-lr-001/training__97__7.072588\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 7.0726 - val_loss: 6.6598\n",
      "Epoch 98/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 6.9460\n",
      "Epoch 00098: val_loss improved from 6.65984 to 6.53775, saving model to models/mlp-adam-lr-001/training__98__6.946665\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 6.9467 - val_loss: 6.5378\n",
      "Epoch 99/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 6.8413\n",
      "Epoch 00099: val_loss improved from 6.53775 to 6.44608, saving model to models/mlp-adam-lr-001/training__99__6.841808\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 6.8418 - val_loss: 6.4461\n",
      "Epoch 100/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 6.6967\n",
      "Epoch 00100: val_loss improved from 6.44608 to 6.32939, saving model to models/mlp-adam-lr-001/training__100__6.696010\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 6.6960 - val_loss: 6.3294\n"
     ]
    }
   ],
   "source": [
    "logdir = \"logs/mlp-adam-lr-001\"\n",
    "checkpoint_path = \"models/mlp-adam-lr-001/training__{epoch:02d}__{loss:.6f}/cp.ckpt\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_best_only=True,\n",
    "                                                 verbose=1)\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=4,monitor='loss')\n",
    "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='loss', factor=0.5, patience=2, verbose=1, mode='auto',\n",
    "    min_delta=0.0001, cooldown=0, min_lr=0\n",
    ")\n",
    "with tf.device('/device:GPU:0'):\n",
    "    model1.fit(\n",
    "        train_dataset, \n",
    "        epochs=100, \n",
    "        validation_data=val_dataset,\n",
    "        callbacks=[tensorboard_callback, cp_callback,early_stop_callback,reduce_lr_callback],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "863795.0651085955\n"
     ]
    }
   ],
   "source": [
    "average = get_average(val_dataset,res,details=False)\n",
    "print(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1432), started 0:06:45 ago. (Use '!kill 1432' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1ab0550478cc3d10\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1ab0550478cc3d10\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP , `Adam`, Learning rate `0.05`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "                learning_rate=0.05,\n",
    "                ),\n",
    "    loss='MSLE',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Layer dense_44 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "  2/679 [..............................] - ETA: 10:39 - loss: 179.3206WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0175s vs `on_train_batch_end` time: 1.8703s). Check your callbacks.\n",
      "668/679 [============================>.] - ETA: 0s - loss: 59.4735\n",
      "Epoch 00001: val_loss improved from inf to 38.29306, saving model to models/mlp-adam-lr-05/training__01__59.158943\\cp.ckpt\n",
      "679/679 [==============================] - 5s 7ms/step - loss: 59.1589 - val_loss: 38.2931\n",
      "Epoch 2/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 32.7400\n",
      "Epoch 00002: val_loss improved from 38.29306 to 27.01051, saving model to models/mlp-adam-lr-05/training__02__32.669804\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 32.6698 - val_loss: 27.0105\n",
      "Epoch 3/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 24.5968\n",
      "Epoch 00003: val_loss improved from 27.01051 to 21.01788, saving model to models/mlp-adam-lr-05/training__03__24.534382\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 24.5344 - val_loss: 21.0179\n",
      "Epoch 4/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 19.5225\n",
      "Epoch 00004: val_loss improved from 21.01788 to 16.94985, saving model to models/mlp-adam-lr-05/training__04__19.511017\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 19.5110 - val_loss: 16.9499\n",
      "Epoch 5/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 16.0215\n",
      "Epoch 00005: val_loss improved from 16.94985 to 13.91426, saving model to models/mlp-adam-lr-05/training__05__16.004530\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 16.0045 - val_loss: 13.9143\n",
      "Epoch 6/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 13.1828\n",
      "Epoch 00006: val_loss improved from 13.91426 to 11.48878, saving model to models/mlp-adam-lr-05/training__06__13.160518\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 13.1605 - val_loss: 11.4888\n",
      "Epoch 7/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 10.9807\n",
      "Epoch 00007: val_loss improved from 11.48878 to 9.47646, saving model to models/mlp-adam-lr-05/training__07__10.971273\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.9713 - val_loss: 9.4765\n",
      "Epoch 8/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 9.0729\n",
      "Epoch 00008: val_loss improved from 9.47646 to 7.79616, saving model to models/mlp-adam-lr-05/training__08__9.066720\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.0667 - val_loss: 7.7962\n",
      "Epoch 9/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 7.4755\n",
      "Epoch 00009: val_loss improved from 7.79616 to 6.37062, saving model to models/mlp-adam-lr-05/training__09__7.472095\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 7.4721 - val_loss: 6.3706\n",
      "Epoch 10/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 6.1175\n",
      "Epoch 00010: val_loss improved from 6.37062 to 5.14832, saving model to models/mlp-adam-lr-05/training__10__6.117142\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 6.1171 - val_loss: 5.1483\n",
      "Epoch 11/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 4.9633\n",
      "Epoch 00011: val_loss improved from 5.14832 to 4.11006, saving model to models/mlp-adam-lr-05/training__11__4.954731\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 4.9547 - val_loss: 4.1101\n",
      "Epoch 12/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 3.9660\n",
      "Epoch 00012: val_loss improved from 4.11006 to 3.22084, saving model to models/mlp-adam-lr-05/training__12__3.960029\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 3.9600 - val_loss: 3.2208\n",
      "Epoch 13/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 3.1341\n",
      "Epoch 00013: val_loss improved from 3.22084 to 2.50051, saving model to models/mlp-adam-lr-05/training__13__3.134117\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 3.1341 - val_loss: 2.5005\n",
      "Epoch 14/100\n",
      "671/679 [============================>.] - ETA: 0s - loss: 2.4526\n",
      "Epoch 00014: val_loss improved from 2.50051 to 1.90368, saving model to models/mlp-adam-lr-05/training__14__2.448496\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 2.4485 - val_loss: 1.9037\n",
      "Epoch 15/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 1.8766\n",
      "Epoch 00015: val_loss improved from 1.90368 to 1.41441, saving model to models/mlp-adam-lr-05/training__15__1.874671\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 1.8747 - val_loss: 1.4144\n",
      "Epoch 16/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 1.4196\n",
      "Epoch 00016: val_loss improved from 1.41441 to 1.04026, saving model to models/mlp-adam-lr-05/training__16__1.419341\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 1.4193 - val_loss: 1.0403\n",
      "Epoch 17/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 1.0613\n",
      "Epoch 00017: val_loss improved from 1.04026 to 0.74201, saving model to models/mlp-adam-lr-05/training__17__1.059967\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 1.0600 - val_loss: 0.7420\n",
      "Epoch 18/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 0.7831\n",
      "Epoch 00018: val_loss improved from 0.74201 to 0.52482, saving model to models/mlp-adam-lr-05/training__18__0.780877\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.7809 - val_loss: 0.5248\n",
      "Epoch 19/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 0.5635\n",
      "Epoch 00019: val_loss improved from 0.52482 to 0.35971, saving model to models/mlp-adam-lr-05/training__19__0.563512\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.5635 - val_loss: 0.3597\n",
      "Epoch 20/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 0.4206\n",
      "Epoch 00020: val_loss improved from 0.35971 to 0.24221, saving model to models/mlp-adam-lr-05/training__20__0.420548\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.4205 - val_loss: 0.2422\n",
      "Epoch 21/100\n",
      "674/679 [============================>.] - ETA: 0s - loss: 0.3150\n",
      "Epoch 00021: val_loss improved from 0.24221 to 0.17193, saving model to models/mlp-adam-lr-05/training__21__0.314963\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.3150 - val_loss: 0.1719\n",
      "Epoch 22/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 0.2445\n",
      "Epoch 00022: val_loss improved from 0.17193 to 0.12011, saving model to models/mlp-adam-lr-05/training__22__0.244257\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.2443 - val_loss: 0.1201\n",
      "Epoch 23/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 0.2052\n",
      "Epoch 00023: val_loss improved from 0.12011 to 0.09786, saving model to models/mlp-adam-lr-05/training__23__0.205421\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.2054 - val_loss: 0.0979\n",
      "Epoch 24/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 0.1799\n",
      "Epoch 00024: val_loss improved from 0.09786 to 0.09127, saving model to models/mlp-adam-lr-05/training__24__0.179520\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1795 - val_loss: 0.0913\n",
      "Epoch 25/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 0.1668\n",
      "Epoch 00025: val_loss improved from 0.09127 to 0.08499, saving model to models/mlp-adam-lr-05/training__25__0.167397\\cp.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1674 - val_loss: 0.0850\n",
      "Epoch 26/100\n",
      "674/679 [============================>.] - ETA: 0s - loss: 0.1562\n",
      "Epoch 00026: val_loss did not improve from 0.08499\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1563 - val_loss: 0.0853\n",
      "Epoch 27/100\n",
      "674/679 [============================>.] - ETA: 0s - loss: 0.1573\n",
      "Epoch 00027: val_loss improved from 0.08499 to 0.08228, saving model to models/mlp-adam-lr-05/training__27__0.157590\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1576 - val_loss: 0.0823\n",
      "Epoch 28/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 0.1552\n",
      "Epoch 00028: val_loss did not improve from 0.08228\n",
      "679/679 [==============================] - 2s 4ms/step - loss: 0.1553 - val_loss: 0.0886\n",
      "Epoch 29/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 0.1538\n",
      "Epoch 00029: val_loss improved from 0.08228 to 0.07731, saving model to models/mlp-adam-lr-05/training__29__0.153789\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1538 - val_loss: 0.0773\n",
      "Epoch 30/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 0.1522\n",
      "Epoch 00030: val_loss did not improve from 0.07731\n",
      "679/679 [==============================] - 2s 4ms/step - loss: 0.1523 - val_loss: 0.0865\n",
      "Epoch 31/100\n",
      "676/679 [============================>.] - ETA: 0s - loss: 0.1506\n",
      "Epoch 00031: val_loss did not improve from 0.07731\n",
      "679/679 [==============================] - 2s 4ms/step - loss: 0.1506 - val_loss: 0.0811\n",
      "Epoch 32/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 0.1497\n",
      "Epoch 00032: val_loss did not improve from 0.07731\n",
      "679/679 [==============================] - 2s 4ms/step - loss: 0.1495 - val_loss: 0.0853\n",
      "Epoch 33/100\n",
      "676/679 [============================>.] - ETA: 0s - loss: 0.1477\n",
      "Epoch 00033: val_loss did not improve from 0.07731\n",
      "679/679 [==============================] - 2s 4ms/step - loss: 0.1478 - val_loss: 0.0805\n",
      "Epoch 34/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 0.1459\n",
      "Epoch 00034: val_loss did not improve from 0.07731\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1463 - val_loss: 0.0811\n",
      "Epoch 35/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 0.1441\n",
      "Epoch 00035: val_loss did not improve from 0.07731\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1448 - val_loss: 0.0792\n",
      "Epoch 36/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 0.1429\n",
      "Epoch 00036: val_loss did not improve from 0.07731\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1430 - val_loss: 0.0789\n",
      "Epoch 37/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 0.1454\n",
      "Epoch 00037: val_loss did not improve from 0.07731\n",
      "679/679 [==============================] - 2s 4ms/step - loss: 0.1454 - val_loss: 0.0786\n",
      "Epoch 38/100\n",
      "676/679 [============================>.] - ETA: 0s - loss: 0.1394\n",
      "Epoch 00038: val_loss improved from 0.07731 to 0.07702, saving model to models/mlp-adam-lr-05/training__38__0.139400\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1394 - val_loss: 0.0770\n",
      "Epoch 39/100\n",
      "671/679 [============================>.] - ETA: 0s - loss: 0.1398\n",
      "Epoch 00039: val_loss improved from 0.07702 to 0.07587, saving model to models/mlp-adam-lr-05/training__39__0.139893\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1399 - val_loss: 0.0759\n",
      "Epoch 40/100\n",
      "671/679 [============================>.] - ETA: 0s - loss: 0.1378\n",
      "Epoch 00040: val_loss did not improve from 0.07587\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1379 - val_loss: 0.0799\n",
      "Epoch 41/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 0.1397\n",
      "Epoch 00041: val_loss improved from 0.07587 to 0.07424, saving model to models/mlp-adam-lr-05/training__41__0.139775\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1398 - val_loss: 0.0742\n",
      "Epoch 42/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 0.1470\n",
      "Epoch 00042: val_loss did not improve from 0.07424\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "679/679 [==============================] - 2s 4ms/step - loss: 0.1466 - val_loss: 0.0776\n",
      "Epoch 43/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 0.1377\n",
      "Epoch 00043: val_loss did not improve from 0.07424\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1373 - val_loss: 0.0762\n",
      "Epoch 44/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 0.1354\n",
      "Epoch 00044: val_loss did not improve from 0.07424\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1351 - val_loss: 0.0762\n",
      "Epoch 45/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 0.1365\n",
      "Epoch 00045: val_loss did not improve from 0.07424\n",
      "679/679 [==============================] - 2s 4ms/step - loss: 0.1365 - val_loss: 0.0755\n",
      "Epoch 46/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 0.1339\n",
      "Epoch 00046: val_loss did not improve from 0.07424\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1336 - val_loss: 0.0758\n",
      "Epoch 47/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 0.1365\n",
      "Epoch 00047: val_loss did not improve from 0.07424\n",
      "679/679 [==============================] - 2s 4ms/step - loss: 0.1367 - val_loss: 0.0754\n",
      "Epoch 48/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 0.1386\n",
      "Epoch 00048: val_loss did not improve from 0.07424\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.012500000186264515.\n",
      "679/679 [==============================] - 2s 4ms/step - loss: 0.1388 - val_loss: 0.0769\n",
      "Epoch 49/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 0.1311\n",
      "Epoch 00049: val_loss improved from 0.07424 to 0.07388, saving model to models/mlp-adam-lr-05/training__49__0.130699\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1307 - val_loss: 0.0739\n",
      "Epoch 50/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 0.1309\n",
      "Epoch 00050: val_loss improved from 0.07388 to 0.07282, saving model to models/mlp-adam-lr-05/training__50__0.131064\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1311 - val_loss: 0.0728\n",
      "Epoch 51/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 0.1329\n",
      "Epoch 00051: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0062500000931322575.\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1328 - val_loss: 0.0751\n",
      "Epoch 52/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 0.1371\n",
      "Epoch 00052: val_loss did not improve from 0.07282\n",
      "679/679 [==============================] - 2s 4ms/step - loss: 0.1369 - val_loss: 0.0743\n",
      "Epoch 53/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 0.1341\n",
      "Epoch 00053: val_loss did not improve from 0.07282\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.0031250000465661287.\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1342 - val_loss: 0.0740\n"
     ]
    }
   ],
   "source": [
    "logdir = \"logs/mlp-adam-lr-05\"\n",
    "checkpoint_path = \"models/mlp-adam-lr-05/training__{epoch:02d}__{loss:.6f}/cp.ckpt\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_best_only=True,\n",
    "                                                 verbose=1)\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=4,monitor='loss')\n",
    "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='loss', factor=0.5, patience=2, verbose=1, mode='auto',\n",
    "    min_delta=0.0001, cooldown=0, min_lr=0\n",
    ")\n",
    "with tf.device('/device:GPU:0'):\n",
    "    model.fit(\n",
    "        train_dataset, \n",
    "        epochs=100, \n",
    "        validation_data=val_dataset,\n",
    "        callbacks=[tensorboard_callback, cp_callback,early_stop_callback,reduce_lr_callback],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328816.39917766815\n"
     ]
    }
   ],
   "source": [
    "average = get_average(val_dataset,res,details=False)\n",
    "print(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1432), started 0:04:12 ago. (Use '!kill 1432' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2b67c78ec80c77e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2b67c78ec80c77e\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP , `SGD`, Learning rate `0.05` momentum `0.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(\n",
    "                learning_rate=0.05,\n",
    "                ),\n",
    "    loss='MSLE',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Layer dense_52 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "  2/679 [..............................] - ETA: 8:10 - loss: 175.1552WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0188s vs `on_train_batch_end` time: 1.4444s). Check your callbacks.\n",
      "667/679 [============================>.] - ETA: 0s - loss: 63.1300\n",
      "Epoch 00001: val_loss improved from inf to 46.67272, saving model to models/mlp-sgd-momentum-lr-05/training__01__62.872982\\cp.ckpt\n",
      "679/679 [==============================] - 4s 6ms/step - loss: 62.8730 - val_loss: 46.6727\n",
      "Epoch 2/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 43.1606\n",
      "Epoch 00002: val_loss improved from 46.67272 to 38.53477, saving model to models/mlp-sgd-momentum-lr-05/training__02__43.160645\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 43.1606 - val_loss: 38.5348\n",
      "Epoch 3/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 37.0748\n",
      "Epoch 00003: val_loss improved from 38.53477 to 34.21510, saving model to models/mlp-sgd-momentum-lr-05/training__03__37.049019\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 37.0490 - val_loss: 34.2151\n",
      "Epoch 4/100\n",
      "666/679 [============================>.] - ETA: 0s - loss: 33.6161\n",
      "Epoch 00004: val_loss improved from 34.21510 to 31.33473, saving model to models/mlp-sgd-momentum-lr-05/training__04__33.581173\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 33.5812 - val_loss: 31.3347\n",
      "Epoch 5/100\n",
      "676/679 [============================>.] - ETA: 0s - loss: 31.0675\n",
      "Epoch 00005: val_loss improved from 31.33473 to 29.20480, saving model to models/mlp-sgd-momentum-lr-05/training__05__31.064220\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 31.0642 - val_loss: 29.2048\n",
      "Epoch 6/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 29.1368\n",
      "Epoch 00006: val_loss improved from 29.20480 to 27.52687, saving model to models/mlp-sgd-momentum-lr-05/training__06__29.132336\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 29.1323 - val_loss: 27.5269\n",
      "Epoch 7/100\n",
      "671/679 [============================>.] - ETA: 0s - loss: 27.5619\n",
      "Epoch 00007: val_loss improved from 27.52687 to 26.15834, saving model to models/mlp-sgd-momentum-lr-05/training__07__27.558798\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 27.5588 - val_loss: 26.1583\n",
      "Epoch 8/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 26.3075\n",
      "Epoch 00008: val_loss improved from 26.15834 to 25.00043, saving model to models/mlp-sgd-momentum-lr-05/training__08__26.297453\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 26.2975 - val_loss: 25.0004\n",
      "Epoch 9/100\n",
      "666/679 [============================>.] - ETA: 0s - loss: 25.1802\n",
      "Epoch 00009: val_loss improved from 25.00043 to 24.00512, saving model to models/mlp-sgd-momentum-lr-05/training__09__25.182299\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 25.1823 - val_loss: 24.0051\n",
      "Epoch 10/100\n",
      "671/679 [============================>.] - ETA: 0s - loss: 24.2173\n",
      "Epoch 00010: val_loss improved from 24.00512 to 23.14044, saving model to models/mlp-sgd-momentum-lr-05/training__10__24.215126\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 24.2151 - val_loss: 23.1404\n",
      "Epoch 11/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 23.4480\n",
      "Epoch 00011: val_loss improved from 23.14044 to 22.36829, saving model to models/mlp-sgd-momentum-lr-05/training__11__23.442827\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 23.4428 - val_loss: 22.3683\n",
      "Epoch 12/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 22.7442\n",
      "Epoch 00012: val_loss improved from 22.36829 to 21.67686, saving model to models/mlp-sgd-momentum-lr-05/training__12__22.731436\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 22.7314 - val_loss: 21.6769\n",
      "Epoch 13/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 22.0810\n",
      "Epoch 00013: val_loss improved from 21.67686 to 21.05374, saving model to models/mlp-sgd-momentum-lr-05/training__13__22.073418\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 22.0734 - val_loss: 21.0537\n",
      "Epoch 14/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 21.4307\n",
      "Epoch 00014: val_loss improved from 21.05374 to 20.48746, saving model to models/mlp-sgd-momentum-lr-05/training__14__21.430691\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 21.4307 - val_loss: 20.4875\n",
      "Epoch 15/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 20.9607\n",
      "Epoch 00015: val_loss improved from 20.48746 to 19.96570, saving model to models/mlp-sgd-momentum-lr-05/training__15__20.955187\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 20.9552 - val_loss: 19.9657\n",
      "Epoch 16/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 20.3725\n",
      "Epoch 00016: val_loss improved from 19.96570 to 19.48789, saving model to models/mlp-sgd-momentum-lr-05/training__16__20.366793\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 20.3668 - val_loss: 19.4879\n",
      "Epoch 17/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 19.8752\n",
      "Epoch 00017: val_loss improved from 19.48789 to 19.04343, saving model to models/mlp-sgd-momentum-lr-05/training__17__19.875200\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 19.8752 - val_loss: 19.0434\n",
      "Epoch 18/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 19.4599\n",
      "Epoch 00018: val_loss improved from 19.04343 to 18.63000, saving model to models/mlp-sgd-momentum-lr-05/training__18__19.459721\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 19.4597 - val_loss: 18.6300\n",
      "Epoch 19/100\n",
      "671/679 [============================>.] - ETA: 0s - loss: 19.1151\n",
      "Epoch 00019: val_loss improved from 18.63000 to 18.24109, saving model to models/mlp-sgd-momentum-lr-05/training__19__19.111219\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 19.1112 - val_loss: 18.2411\n",
      "Epoch 20/100\n",
      "664/679 [============================>.] - ETA: 0s - loss: 18.7065\n",
      "Epoch 00020: val_loss improved from 18.24109 to 17.88199, saving model to models/mlp-sgd-momentum-lr-05/training__20__18.692602\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 18.6926 - val_loss: 17.8820\n",
      "Epoch 21/100\n",
      "674/679 [============================>.] - ETA: 0s - loss: 18.2724\n",
      "Epoch 00021: val_loss improved from 17.88199 to 17.53789, saving model to models/mlp-sgd-momentum-lr-05/training__21__18.272652\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 18.2727 - val_loss: 17.5379\n",
      "Epoch 22/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 17.9850\n",
      "Epoch 00022: val_loss improved from 17.53789 to 17.21983, saving model to models/mlp-sgd-momentum-lr-05/training__22__17.983047\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 17.9830 - val_loss: 17.2198\n",
      "Epoch 23/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 17.6106\n",
      "Epoch 00023: val_loss improved from 17.21983 to 16.91577, saving model to models/mlp-sgd-momentum-lr-05/training__23__17.610647\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 17.6106 - val_loss: 16.9158\n",
      "Epoch 24/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 17.4005\n",
      "Epoch 00024: val_loss improved from 16.91577 to 16.62976, saving model to models/mlp-sgd-momentum-lr-05/training__24__17.400513\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 17.4005 - val_loss: 16.6298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 17.0740\n",
      "Epoch 00025: val_loss improved from 16.62976 to 16.35612, saving model to models/mlp-sgd-momentum-lr-05/training__25__17.070515\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 17.0705 - val_loss: 16.3561\n",
      "Epoch 26/100\n",
      "666/679 [============================>.] - ETA: 0s - loss: 16.8178\n",
      "Epoch 00026: val_loss improved from 16.35612 to 16.09419, saving model to models/mlp-sgd-momentum-lr-05/training__26__16.816561\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 16.8166 - val_loss: 16.0942\n",
      "Epoch 27/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 16.5026\n",
      "Epoch 00027: val_loss improved from 16.09419 to 15.84797, saving model to models/mlp-sgd-momentum-lr-05/training__27__16.509924\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 16.5099 - val_loss: 15.8480\n",
      "Epoch 28/100\n",
      "676/679 [============================>.] - ETA: 0s - loss: 16.2334\n",
      "Epoch 00028: val_loss improved from 15.84797 to 15.61179, saving model to models/mlp-sgd-momentum-lr-05/training__28__16.230904\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 16.2309 - val_loss: 15.6118\n",
      "Epoch 29/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 16.0653\n",
      "Epoch 00029: val_loss improved from 15.61179 to 15.38385, saving model to models/mlp-sgd-momentum-lr-05/training__29__16.068268\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 16.0683 - val_loss: 15.3839\n",
      "Epoch 30/100\n",
      "671/679 [============================>.] - ETA: 0s - loss: 15.8027\n",
      "Epoch 00030: val_loss improved from 15.38385 to 15.17119, saving model to models/mlp-sgd-momentum-lr-05/training__30__15.803318\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 15.8033 - val_loss: 15.1712\n",
      "Epoch 31/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 15.6284\n",
      "Epoch 00031: val_loss improved from 15.17119 to 14.96148, saving model to models/mlp-sgd-momentum-lr-05/training__31__15.628390\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 15.6284 - val_loss: 14.9615\n",
      "Epoch 32/100\n",
      "666/679 [============================>.] - ETA: 0s - loss: 15.4344\n",
      "Epoch 00032: val_loss improved from 14.96148 to 14.76142, saving model to models/mlp-sgd-momentum-lr-05/training__32__15.432962\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 15.4330 - val_loss: 14.7614\n",
      "Epoch 33/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 15.1980\n",
      "Epoch 00033: val_loss improved from 14.76142 to 14.56962, saving model to models/mlp-sgd-momentum-lr-05/training__33__15.200781\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 15.2008 - val_loss: 14.5696\n",
      "Epoch 34/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 15.0428\n",
      "Epoch 00034: val_loss improved from 14.56962 to 14.38555, saving model to models/mlp-sgd-momentum-lr-05/training__34__15.044110\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 15.0441 - val_loss: 14.3855\n",
      "Epoch 35/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 14.8760\n",
      "Epoch 00035: val_loss improved from 14.38555 to 14.20639, saving model to models/mlp-sgd-momentum-lr-05/training__35__14.875931\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 14.8759 - val_loss: 14.2064\n",
      "Epoch 36/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 14.6978\n",
      "Epoch 00036: val_loss improved from 14.20639 to 14.03406, saving model to models/mlp-sgd-momentum-lr-05/training__36__14.698066\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 14.6981 - val_loss: 14.0341\n",
      "Epoch 37/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 14.5336\n",
      "Epoch 00037: val_loss improved from 14.03406 to 13.86683, saving model to models/mlp-sgd-momentum-lr-05/training__37__14.533631\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 14.5336 - val_loss: 13.8668\n",
      "Epoch 38/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 14.3643\n",
      "Epoch 00038: val_loss improved from 13.86683 to 13.70410, saving model to models/mlp-sgd-momentum-lr-05/training__38__14.357147\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 14.3571 - val_loss: 13.7041\n",
      "Epoch 39/100\n",
      "676/679 [============================>.] - ETA: 0s - loss: 14.1518\n",
      "Epoch 00039: val_loss improved from 13.70410 to 13.55236, saving model to models/mlp-sgd-momentum-lr-05/training__39__14.149858\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 14.1499 - val_loss: 13.5524\n",
      "Epoch 40/100\n",
      "676/679 [============================>.] - ETA: 0s - loss: 14.0235\n",
      "Epoch 00040: val_loss improved from 13.55236 to 13.39935, saving model to models/mlp-sgd-momentum-lr-05/training__40__14.023316\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 14.0233 - val_loss: 13.3994\n",
      "Epoch 41/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 13.8796\n",
      "Epoch 00041: val_loss improved from 13.39935 to 13.25808, saving model to models/mlp-sgd-momentum-lr-05/training__41__13.883237\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 13.8832 - val_loss: 13.2581\n",
      "Epoch 42/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 13.7208\n",
      "Epoch 00042: val_loss improved from 13.25808 to 13.11397, saving model to models/mlp-sgd-momentum-lr-05/training__42__13.723073\\cp.ckpt\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 13.7231 - val_loss: 13.1140\n",
      "Epoch 43/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 13.5655\n",
      "Epoch 00043: val_loss improved from 13.11397 to 12.97589, saving model to models/mlp-sgd-momentum-lr-05/training__43__13.570460\\cp.ckpt\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 13.5705 - val_loss: 12.9759\n",
      "Epoch 44/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 13.4735\n",
      "Epoch 00044: val_loss improved from 12.97589 to 12.84491, saving model to models/mlp-sgd-momentum-lr-05/training__44__13.473655\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 13.4737 - val_loss: 12.8449\n",
      "Epoch 45/100\n",
      "660/679 [============================>.] - ETA: 0s - loss: 13.2905\n",
      "Epoch 00045: val_loss improved from 12.84491 to 12.71659, saving model to models/mlp-sgd-momentum-lr-05/training__45__13.290093\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 13.2901 - val_loss: 12.7166\n",
      "Epoch 46/100\n",
      "662/679 [============================>.] - ETA: 0s - loss: 13.1277\n",
      "Epoch 00046: val_loss improved from 12.71659 to 12.59569, saving model to models/mlp-sgd-momentum-lr-05/training__46__13.135390\\cp.ckpt\n",
      "679/679 [==============================] - 2s 4ms/step - loss: 13.1354 - val_loss: 12.5957\n",
      "Epoch 47/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 13.0906\n",
      "Epoch 00047: val_loss improved from 12.59569 to 12.46216, saving model to models/mlp-sgd-momentum-lr-05/training__47__13.091792\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 13.0918 - val_loss: 12.4622\n",
      "Epoch 48/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 12.9137\n",
      "Epoch 00048: val_loss improved from 12.46216 to 12.33152, saving model to models/mlp-sgd-momentum-lr-05/training__48__12.914786\\cp.ckpt\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 12.9148 - val_loss: 12.3315\n",
      "Epoch 49/100\n",
      "666/679 [============================>.] - ETA: 0s - loss: 12.7808\n",
      "Epoch 00049: val_loss improved from 12.33152 to 12.22891, saving model to models/mlp-sgd-momentum-lr-05/training__49__12.782813\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 12.7828 - val_loss: 12.2289\n",
      "Epoch 50/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 12.6686\n",
      "Epoch 00050: val_loss improved from 12.22891 to 12.12053, saving model to models/mlp-sgd-momentum-lr-05/training__50__12.669811\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 12.6698 - val_loss: 12.1205\n",
      "Epoch 51/100\n",
      "666/679 [============================>.] - ETA: 0s - loss: 12.5700\n",
      "Epoch 00051: val_loss improved from 12.12053 to 12.00689, saving model to models/mlp-sgd-momentum-lr-05/training__51__12.572319\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 12.5723 - val_loss: 12.0069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100\n",
      "662/679 [============================>.] - ETA: 0s - loss: 12.4652\n",
      "Epoch 00052: val_loss improved from 12.00689 to 11.90065, saving model to models/mlp-sgd-momentum-lr-05/training__52__12.467866\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 12.4679 - val_loss: 11.9007\n",
      "Epoch 53/100\n",
      "671/679 [============================>.] - ETA: 0s - loss: 12.3667\n",
      "Epoch 00053: val_loss improved from 11.90065 to 11.79417, saving model to models/mlp-sgd-momentum-lr-05/training__53__12.372562\\cp.ckpt\n",
      "679/679 [==============================] - 2s 4ms/step - loss: 12.3726 - val_loss: 11.7942\n",
      "Epoch 54/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 12.2900\n",
      "Epoch 00054: val_loss improved from 11.79417 to 11.69438, saving model to models/mlp-sgd-momentum-lr-05/training__54__12.292661\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 12.2927 - val_loss: 11.6944\n",
      "Epoch 55/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 12.1367\n",
      "Epoch 00055: val_loss improved from 11.69438 to 11.59486, saving model to models/mlp-sgd-momentum-lr-05/training__55__12.133399\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 12.1334 - val_loss: 11.5949\n",
      "Epoch 56/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 12.0563\n",
      "Epoch 00056: val_loss improved from 11.59486 to 11.50195, saving model to models/mlp-sgd-momentum-lr-05/training__56__12.056975\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 12.0570 - val_loss: 11.5019\n",
      "Epoch 57/100\n",
      "666/679 [============================>.] - ETA: 0s - loss: 11.9535\n",
      "Epoch 00057: val_loss improved from 11.50195 to 11.40223, saving model to models/mlp-sgd-momentum-lr-05/training__57__11.956650\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 11.9566 - val_loss: 11.4022\n",
      "Epoch 58/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 11.8376\n",
      "Epoch 00058: val_loss improved from 11.40223 to 11.30954, saving model to models/mlp-sgd-momentum-lr-05/training__58__11.839188\\cp.ckpt\n",
      "679/679 [==============================] - 2s 4ms/step - loss: 11.8392 - val_loss: 11.3095\n",
      "Epoch 59/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 11.7665\n",
      "Epoch 00059: val_loss improved from 11.30954 to 11.21679, saving model to models/mlp-sgd-momentum-lr-05/training__59__11.764653\\cp.ckpt\n",
      "679/679 [==============================] - 2s 4ms/step - loss: 11.7647 - val_loss: 11.2168\n",
      "Epoch 60/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 11.6645\n",
      "Epoch 00060: val_loss improved from 11.21679 to 11.13130, saving model to models/mlp-sgd-momentum-lr-05/training__60__11.664489\\cp.ckpt\n",
      "679/679 [==============================] - 2s 4ms/step - loss: 11.6645 - val_loss: 11.1313\n",
      "Epoch 61/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 11.5662\n",
      "Epoch 00061: val_loss improved from 11.13130 to 11.04132, saving model to models/mlp-sgd-momentum-lr-05/training__61__11.566721\\cp.ckpt\n",
      "679/679 [==============================] - 2s 4ms/step - loss: 11.5667 - val_loss: 11.0413\n",
      "Epoch 62/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 11.4863\n",
      "Epoch 00062: val_loss improved from 11.04132 to 10.94967, saving model to models/mlp-sgd-momentum-lr-05/training__62__11.491848\\cp.ckpt\n",
      "679/679 [==============================] - 2s 4ms/step - loss: 11.4918 - val_loss: 10.9497\n",
      "Epoch 63/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 11.3466\n",
      "Epoch 00063: val_loss improved from 10.94967 to 10.87090, saving model to models/mlp-sgd-momentum-lr-05/training__63__11.349810\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 11.3498 - val_loss: 10.8709\n",
      "Epoch 64/100\n",
      "661/679 [============================>.] - ETA: 0s - loss: 11.3089\n",
      "Epoch 00064: val_loss improved from 10.87090 to 10.79321, saving model to models/mlp-sgd-momentum-lr-05/training__64__11.312295\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 11.3123 - val_loss: 10.7932\n",
      "Epoch 65/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 11.2120\n",
      "Epoch 00065: val_loss improved from 10.79321 to 10.71377, saving model to models/mlp-sgd-momentum-lr-05/training__65__11.208953\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 11.2090 - val_loss: 10.7138\n",
      "Epoch 66/100\n",
      "663/679 [============================>.] - ETA: 0s - loss: 11.1217\n",
      "Epoch 00066: val_loss improved from 10.71377 to 10.62884, saving model to models/mlp-sgd-momentum-lr-05/training__66__11.121838\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 11.1218 - val_loss: 10.6288\n",
      "Epoch 67/100\n",
      "663/679 [============================>.] - ETA: 0s - loss: 11.0982\n",
      "Epoch 00067: val_loss improved from 10.62884 to 10.55174, saving model to models/mlp-sgd-momentum-lr-05/training__67__11.098578\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 11.0986 - val_loss: 10.5517\n",
      "Epoch 68/100\n",
      "660/679 [============================>.] - ETA: 0s - loss: 11.0186\n",
      "Epoch 00068: val_loss improved from 10.55174 to 10.48219, saving model to models/mlp-sgd-momentum-lr-05/training__68__11.020136\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 11.0201 - val_loss: 10.4822\n",
      "Epoch 69/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 10.9014\n",
      "Epoch 00069: val_loss improved from 10.48219 to 10.40281, saving model to models/mlp-sgd-momentum-lr-05/training__69__10.901392\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.9014 - val_loss: 10.4028\n",
      "Epoch 70/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 10.8300\n",
      "Epoch 00070: val_loss improved from 10.40281 to 10.32451, saving model to models/mlp-sgd-momentum-lr-05/training__70__10.830044\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.8300 - val_loss: 10.3245\n",
      "Epoch 71/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 10.8018\n",
      "Epoch 00071: val_loss improved from 10.32451 to 10.25662, saving model to models/mlp-sgd-momentum-lr-05/training__71__10.804251\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.8043 - val_loss: 10.2566\n",
      "Epoch 72/100\n",
      "674/679 [============================>.] - ETA: 0s - loss: 10.7013\n",
      "Epoch 00072: val_loss improved from 10.25662 to 10.18690, saving model to models/mlp-sgd-momentum-lr-05/training__72__10.704562\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.7046 - val_loss: 10.1869\n",
      "Epoch 73/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 10.6014\n",
      "Epoch 00073: val_loss improved from 10.18690 to 10.11498, saving model to models/mlp-sgd-momentum-lr-05/training__73__10.603714\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.6037 - val_loss: 10.1150\n",
      "Epoch 74/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 10.5448\n",
      "Epoch 00074: val_loss improved from 10.11498 to 10.04688, saving model to models/mlp-sgd-momentum-lr-05/training__74__10.545947\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.5459 - val_loss: 10.0469\n",
      "Epoch 75/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 10.4747\n",
      "Epoch 00075: val_loss improved from 10.04688 to 9.98005, saving model to models/mlp-sgd-momentum-lr-05/training__75__10.477567\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.4776 - val_loss: 9.9800\n",
      "Epoch 76/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 10.4433\n",
      "Epoch 00076: val_loss improved from 9.98005 to 9.91489, saving model to models/mlp-sgd-momentum-lr-05/training__76__10.445433\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.4454 - val_loss: 9.9149\n",
      "Epoch 77/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 10.3475\n",
      "Epoch 00077: val_loss improved from 9.91489 to 9.85583, saving model to models/mlp-sgd-momentum-lr-05/training__77__10.353768\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.3538 - val_loss: 9.8558\n",
      "Epoch 78/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 10.2744\n",
      "Epoch 00078: val_loss improved from 9.85583 to 9.78741, saving model to models/mlp-sgd-momentum-lr-05/training__78__10.276313\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.2763 - val_loss: 9.7874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 10.1793\n",
      "Epoch 00079: val_loss improved from 9.78741 to 9.72956, saving model to models/mlp-sgd-momentum-lr-05/training__79__10.179235\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.1792 - val_loss: 9.7296\n",
      "Epoch 80/100\n",
      "666/679 [============================>.] - ETA: 0s - loss: 10.1914\n",
      "Epoch 00080: val_loss improved from 9.72956 to 9.66186, saving model to models/mlp-sgd-momentum-lr-05/training__80__10.186837\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.1868 - val_loss: 9.6619\n",
      "Epoch 81/100\n",
      "671/679 [============================>.] - ETA: 0s - loss: 10.1078\n",
      "Epoch 00081: val_loss improved from 9.66186 to 9.60229, saving model to models/mlp-sgd-momentum-lr-05/training__81__10.111694\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.1117 - val_loss: 9.6023\n",
      "Epoch 82/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 10.0330\n",
      "Epoch 00082: val_loss improved from 9.60229 to 9.54548, saving model to models/mlp-sgd-momentum-lr-05/training__82__10.028305\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.0283 - val_loss: 9.5455\n",
      "Epoch 83/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 9.9896\n",
      "Epoch 00083: val_loss improved from 9.54548 to 9.48415, saving model to models/mlp-sgd-momentum-lr-05/training__83__9.989568\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.9896 - val_loss: 9.4841\n",
      "Epoch 84/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 9.8996\n",
      "Epoch 00084: val_loss improved from 9.48415 to 9.43003, saving model to models/mlp-sgd-momentum-lr-05/training__84__9.900653\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.9007 - val_loss: 9.4300\n",
      "Epoch 85/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 9.8412\n",
      "Epoch 00085: val_loss improved from 9.43003 to 9.37271, saving model to models/mlp-sgd-momentum-lr-05/training__85__9.843307\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.8433 - val_loss: 9.3727\n",
      "Epoch 86/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 9.7818\n",
      "Epoch 00086: val_loss improved from 9.37271 to 9.31616, saving model to models/mlp-sgd-momentum-lr-05/training__86__9.778788\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.7788 - val_loss: 9.3162\n",
      "Epoch 87/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 9.7332\n",
      "Epoch 00087: val_loss improved from 9.31616 to 9.26358, saving model to models/mlp-sgd-momentum-lr-05/training__87__9.734178\\cp.ckpt\n",
      "679/679 [==============================] - 2s 4ms/step - loss: 9.7342 - val_loss: 9.2636\n",
      "Epoch 88/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 9.6649\n",
      "Epoch 00088: val_loss improved from 9.26358 to 9.20499, saving model to models/mlp-sgd-momentum-lr-05/training__88__9.666770\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.6668 - val_loss: 9.2050\n",
      "Epoch 89/100\n",
      "671/679 [============================>.] - ETA: 0s - loss: 9.6116\n",
      "Epoch 00089: val_loss improved from 9.20499 to 9.15576, saving model to models/mlp-sgd-momentum-lr-05/training__89__9.610763\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.6108 - val_loss: 9.1558\n",
      "Epoch 90/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 9.5928\n",
      "Epoch 00090: val_loss improved from 9.15576 to 9.10110, saving model to models/mlp-sgd-momentum-lr-05/training__90__9.591083\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.5911 - val_loss: 9.1011\n",
      "Epoch 91/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 9.5043\n",
      "Epoch 00091: val_loss improved from 9.10110 to 9.04925, saving model to models/mlp-sgd-momentum-lr-05/training__91__9.505349\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.5053 - val_loss: 9.0492\n",
      "Epoch 92/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 9.4687\n",
      "Epoch 00092: val_loss improved from 9.04925 to 8.99883, saving model to models/mlp-sgd-momentum-lr-05/training__92__9.471497\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.4715 - val_loss: 8.9988\n",
      "Epoch 93/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 9.4392\n",
      "Epoch 00093: val_loss improved from 8.99883 to 8.94971, saving model to models/mlp-sgd-momentum-lr-05/training__93__9.442706\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.4427 - val_loss: 8.9497\n",
      "Epoch 94/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 9.3869\n",
      "Epoch 00094: val_loss improved from 8.94971 to 8.89923, saving model to models/mlp-sgd-momentum-lr-05/training__94__9.394474\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.3945 - val_loss: 8.8992\n",
      "Epoch 95/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 9.2787\n",
      "Epoch 00095: val_loss improved from 8.89923 to 8.84760, saving model to models/mlp-sgd-momentum-lr-05/training__95__9.277211\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.2772 - val_loss: 8.8476\n",
      "Epoch 96/100\n",
      "674/679 [============================>.] - ETA: 0s - loss: 9.2978\n",
      "Epoch 00096: val_loss improved from 8.84760 to 8.79991, saving model to models/mlp-sgd-momentum-lr-05/training__96__9.295239\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.2952 - val_loss: 8.7999\n",
      "Epoch 97/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 9.2294\n",
      "Epoch 00097: val_loss improved from 8.79991 to 8.75725, saving model to models/mlp-sgd-momentum-lr-05/training__97__9.229528\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.2295 - val_loss: 8.7573\n",
      "Epoch 98/100\n",
      "676/679 [============================>.] - ETA: 0s - loss: 9.1458\n",
      "Epoch 00098: val_loss improved from 8.75725 to 8.70650, saving model to models/mlp-sgd-momentum-lr-05/training__98__9.146422\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.1464 - val_loss: 8.7065\n",
      "Epoch 99/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 9.1031\n",
      "Epoch 00099: val_loss improved from 8.70650 to 8.65965, saving model to models/mlp-sgd-momentum-lr-05/training__99__9.102956\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.1030 - val_loss: 8.6597\n",
      "Epoch 100/100\n",
      "661/679 [============================>.] - ETA: 0s - loss: 9.0410\n",
      "Epoch 00100: val_loss improved from 8.65965 to 8.61752, saving model to models/mlp-sgd-momentum-lr-05/training__100__9.041622\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.0416 - val_loss: 8.6175\n"
     ]
    }
   ],
   "source": [
    "logdir = \"logs/mlp-sgd-momentum-0-lr-05\"\n",
    "checkpoint_path = \"models/mlp-sgd-momentum-0-lr-05/training__{epoch:02d}__{loss:.6f}/cp.ckpt\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_best_only=True,\n",
    "                                                 verbose=1)\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=4,monitor='loss')\n",
    "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='loss', factor=0.5, patience=2, verbose=1, mode='auto',\n",
    "    min_delta=0.0001, cooldown=0, min_lr=0\n",
    ")\n",
    "with tf.device('/device:GPU:0'):\n",
    "    model.fit(\n",
    "        train_dataset, \n",
    "        epochs=100, \n",
    "        validation_data=val_dataset,\n",
    "        callbacks=[tensorboard_callback, cp_callback,early_stop_callback,reduce_lr_callback],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "886747.0076241423\n"
     ]
    }
   ],
   "source": [
    "average = get_average(val_dataset,res,details=False)\n",
    "print(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1432), started 0:13:28 ago. (Use '!kill 1432' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c7f4cf6162dc39be\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c7f4cf6162dc39be\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP , `SGD`, Learning rate `0.05` momentum `0.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(\n",
    "                learning_rate=0.05,\n",
    "                momentum=0.2\n",
    "                ),\n",
    "    loss='MSLE',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Layer dense_56 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "  2/679 [..............................] - ETA: 4:55 - loss: 167.8057WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_begin` time: 0.0171s). Check your callbacks.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.8541s). Check your callbacks.\n",
      "667/679 [============================>.] - ETA: 0s - loss: 59.5581\n",
      "Epoch 00001: val_loss improved from inf to 43.94683, saving model to models/mlp-sgd-momentum-02-lr-05/training__01__59.302155\\cp.ckpt\n",
      "679/679 [==============================] - 4s 6ms/step - loss: 59.3022 - val_loss: 43.9468\n",
      "Epoch 2/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 40.4067\n",
      "Epoch 00002: val_loss improved from 43.94683 to 36.11645, saving model to models/mlp-sgd-momentum-02-lr-05/training__02__40.403038\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 40.4030 - val_loss: 36.1164\n",
      "Epoch 3/100\n",
      "674/679 [============================>.] - ETA: 0s - loss: 34.6654\n",
      "Epoch 00003: val_loss improved from 36.11645 to 31.97486, saving model to models/mlp-sgd-momentum-02-lr-05/training__03__34.653297\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 34.6533 - val_loss: 31.9749\n",
      "Epoch 4/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 31.2870\n",
      "Epoch 00004: val_loss improved from 31.97486 to 29.20742, saving model to models/mlp-sgd-momentum-02-lr-05/training__04__31.263048\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 31.2630 - val_loss: 29.2074\n",
      "Epoch 5/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 28.8244\n",
      "Epoch 00005: val_loss improved from 29.20742 to 27.16191, saving model to models/mlp-sgd-momentum-02-lr-05/training__05__28.824455\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 28.8245 - val_loss: 27.1619\n",
      "Epoch 6/100\n",
      "664/679 [============================>.] - ETA: 0s - loss: 27.0068\n",
      "Epoch 00006: val_loss improved from 27.16191 to 25.55551, saving model to models/mlp-sgd-momentum-02-lr-05/training__06__26.985880\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 26.9859 - val_loss: 25.5555\n",
      "Epoch 7/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 25.5915\n",
      "Epoch 00007: val_loss improved from 25.55551 to 24.24428, saving model to models/mlp-sgd-momentum-02-lr-05/training__07__25.587137\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 25.5871 - val_loss: 24.2443\n",
      "Epoch 8/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 24.3114\n",
      "Epoch 00008: val_loss improved from 24.24428 to 23.14299, saving model to models/mlp-sgd-momentum-02-lr-05/training__08__24.305796\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 24.3058 - val_loss: 23.1430\n",
      "Epoch 9/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 23.3345\n",
      "Epoch 00009: val_loss improved from 23.14299 to 22.19062, saving model to models/mlp-sgd-momentum-02-lr-05/training__09__23.334520\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 23.3345 - val_loss: 22.1906\n",
      "Epoch 10/100\n",
      "671/679 [============================>.] - ETA: 0s - loss: 22.4405\n",
      "Epoch 00010: val_loss improved from 22.19062 to 21.35743, saving model to models/mlp-sgd-momentum-02-lr-05/training__10__22.430285\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 22.4303 - val_loss: 21.3574\n",
      "Epoch 11/100\n",
      "671/679 [============================>.] - ETA: 0s - loss: 21.5638\n",
      "Epoch 00011: val_loss improved from 21.35743 to 20.62767, saving model to models/mlp-sgd-momentum-02-lr-05/training__11__21.560957\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 21.5610 - val_loss: 20.6277\n",
      "Epoch 12/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 20.8511\n",
      "Epoch 00012: val_loss improved from 20.62767 to 19.96803, saving model to models/mlp-sgd-momentum-02-lr-05/training__12__20.849894\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 20.8499 - val_loss: 19.9680\n",
      "Epoch 13/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 20.2713\n",
      "Epoch 00013: val_loss improved from 19.96803 to 19.37577, saving model to models/mlp-sgd-momentum-02-lr-05/training__13__20.269230\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 20.2692 - val_loss: 19.3758\n",
      "Epoch 14/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 19.6976\n",
      "Epoch 00014: val_loss improved from 19.37577 to 18.83572, saving model to models/mlp-sgd-momentum-02-lr-05/training__14__19.696012\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 19.6960 - val_loss: 18.8357\n",
      "Epoch 15/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 19.1275\n",
      "Epoch 00015: val_loss improved from 18.83572 to 18.34000, saving model to models/mlp-sgd-momentum-02-lr-05/training__15__19.127459\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 19.1275 - val_loss: 18.3400\n",
      "Epoch 16/100\n",
      "674/679 [============================>.] - ETA: 0s - loss: 18.7080\n",
      "Epoch 00016: val_loss improved from 18.34000 to 17.88735, saving model to models/mlp-sgd-momentum-02-lr-05/training__16__18.712275\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 18.7123 - val_loss: 17.8874\n",
      "Epoch 17/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 18.2019\n",
      "Epoch 00017: val_loss improved from 17.88735 to 17.46411, saving model to models/mlp-sgd-momentum-02-lr-05/training__17__18.199139\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 18.1991 - val_loss: 17.4641\n",
      "Epoch 18/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 17.8267\n",
      "Epoch 00018: val_loss improved from 17.46411 to 17.06904, saving model to models/mlp-sgd-momentum-02-lr-05/training__18__17.829180\\cp.ckpt\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 17.8292 - val_loss: 17.0690\n",
      "Epoch 19/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 17.5062\n",
      "Epoch 00019: val_loss improved from 17.06904 to 16.70100, saving model to models/mlp-sgd-momentum-02-lr-05/training__19__17.506353\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 17.5064 - val_loss: 16.7010\n",
      "Epoch 20/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 17.0618\n",
      "Epoch 00020: val_loss improved from 16.70100 to 16.35868, saving model to models/mlp-sgd-momentum-02-lr-05/training__20__17.063841\\cp.ckpt\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 17.0638 - val_loss: 16.3587\n",
      "Epoch 21/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 16.7427\n",
      "Epoch 00021: val_loss improved from 16.35868 to 16.03454, saving model to models/mlp-sgd-momentum-02-lr-05/training__21__16.741964\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 16.7420 - val_loss: 16.0345\n",
      "Epoch 22/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 16.4056\n",
      "Epoch 00022: val_loss improved from 16.03454 to 15.73336, saving model to models/mlp-sgd-momentum-02-lr-05/training__22__16.405647\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 16.4056 - val_loss: 15.7334\n",
      "Epoch 23/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 16.1575\n",
      "Epoch 00023: val_loss improved from 15.73336 to 15.44386, saving model to models/mlp-sgd-momentum-02-lr-05/training__23__16.159103\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 16.1591 - val_loss: 15.4439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 15.8003\n",
      "Epoch 00024: val_loss improved from 15.44386 to 15.17304, saving model to models/mlp-sgd-momentum-02-lr-05/training__24__15.797124\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 15.7971 - val_loss: 15.1730\n",
      "Epoch 25/100\n",
      "676/679 [============================>.] - ETA: 0s - loss: 15.5756\n",
      "Epoch 00025: val_loss improved from 15.17304 to 14.91199, saving model to models/mlp-sgd-momentum-02-lr-05/training__25__15.575999\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 15.5760 - val_loss: 14.9120\n",
      "Epoch 26/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 15.3003\n",
      "Epoch 00026: val_loss improved from 14.91199 to 14.66634, saving model to models/mlp-sgd-momentum-02-lr-05/training__26__15.295382\\cp.ckpt\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 15.2954 - val_loss: 14.6663\n",
      "Epoch 27/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 15.0778\n",
      "Epoch 00027: val_loss improved from 14.66634 to 14.43072, saving model to models/mlp-sgd-momentum-02-lr-05/training__27__15.078211\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 15.0782 - val_loss: 14.4307\n",
      "Epoch 28/100\n",
      "676/679 [============================>.] - ETA: 0s - loss: 14.8232\n",
      "Epoch 00028: val_loss improved from 14.43072 to 14.20746, saving model to models/mlp-sgd-momentum-02-lr-05/training__28__14.824948\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 14.8249 - val_loss: 14.2075\n",
      "Epoch 29/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 14.6191\n",
      "Epoch 00029: val_loss improved from 14.20746 to 13.99398, saving model to models/mlp-sgd-momentum-02-lr-05/training__29__14.621522\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 14.6215 - val_loss: 13.9940\n",
      "Epoch 30/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 14.3737\n",
      "Epoch 00030: val_loss improved from 13.99398 to 13.78786, saving model to models/mlp-sgd-momentum-02-lr-05/training__30__14.374545\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 14.3745 - val_loss: 13.7879\n",
      "Epoch 31/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 14.1957\n",
      "Epoch 00031: val_loss improved from 13.78786 to 13.59143, saving model to models/mlp-sgd-momentum-02-lr-05/training__31__14.192312\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 14.1923 - val_loss: 13.5914\n",
      "Epoch 32/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 14.0188\n",
      "Epoch 00032: val_loss improved from 13.59143 to 13.40466, saving model to models/mlp-sgd-momentum-02-lr-05/training__32__14.015251\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 14.0153 - val_loss: 13.4047\n",
      "Epoch 33/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 13.8433\n",
      "Epoch 00033: val_loss improved from 13.40466 to 13.22362, saving model to models/mlp-sgd-momentum-02-lr-05/training__33__13.844505\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 13.8445 - val_loss: 13.2236\n",
      "Epoch 34/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 13.5906\n",
      "Epoch 00034: val_loss improved from 13.22362 to 13.04974, saving model to models/mlp-sgd-momentum-02-lr-05/training__34__13.591981\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 13.5920 - val_loss: 13.0497\n",
      "Epoch 35/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 13.4669\n",
      "Epoch 00035: val_loss improved from 13.04974 to 12.87976, saving model to models/mlp-sgd-momentum-02-lr-05/training__35__13.469865\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 13.4699 - val_loss: 12.8798\n",
      "Epoch 36/100\n",
      "674/679 [============================>.] - ETA: 0s - loss: 13.2779\n",
      "Epoch 00036: val_loss improved from 12.87976 to 12.71790, saving model to models/mlp-sgd-momentum-02-lr-05/training__36__13.280808\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 13.2808 - val_loss: 12.7179\n",
      "Epoch 37/100\n",
      "663/679 [============================>.] - ETA: 0s - loss: 13.1411\n",
      "Epoch 00037: val_loss improved from 12.71790 to 12.56152, saving model to models/mlp-sgd-momentum-02-lr-05/training__37__13.137547\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 13.1375 - val_loss: 12.5615\n",
      "Epoch 38/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 13.0283\n",
      "Epoch 00038: val_loss improved from 12.56152 to 12.40623, saving model to models/mlp-sgd-momentum-02-lr-05/training__38__13.022415\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 13.0224 - val_loss: 12.4062\n",
      "Epoch 39/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 12.7721\n",
      "Epoch 00039: val_loss improved from 12.40623 to 12.26222, saving model to models/mlp-sgd-momentum-02-lr-05/training__39__12.770275\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 12.7703 - val_loss: 12.2622\n",
      "Epoch 40/100\n",
      "666/679 [============================>.] - ETA: 0s - loss: 12.6900\n",
      "Epoch 00040: val_loss improved from 12.26222 to 12.11866, saving model to models/mlp-sgd-momentum-02-lr-05/training__40__12.690027\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 12.6900 - val_loss: 12.1187\n",
      "Epoch 41/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 12.5205\n",
      "Epoch 00041: val_loss improved from 12.11866 to 11.98248, saving model to models/mlp-sgd-momentum-02-lr-05/training__41__12.517798\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 12.5178 - val_loss: 11.9825\n",
      "Epoch 42/100\n",
      "671/679 [============================>.] - ETA: 0s - loss: 12.3363\n",
      "Epoch 00042: val_loss improved from 11.98248 to 11.85353, saving model to models/mlp-sgd-momentum-02-lr-05/training__42__12.338819\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 12.3388 - val_loss: 11.8535\n",
      "Epoch 43/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 12.2507\n",
      "Epoch 00043: val_loss improved from 11.85353 to 11.72086, saving model to models/mlp-sgd-momentum-02-lr-05/training__43__12.250733\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 12.2507 - val_loss: 11.7209\n",
      "Epoch 44/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 12.0964\n",
      "Epoch 00044: val_loss improved from 11.72086 to 11.59549, saving model to models/mlp-sgd-momentum-02-lr-05/training__44__12.099447\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 12.0994 - val_loss: 11.5955\n",
      "Epoch 45/100\n",
      "664/679 [============================>.] - ETA: 0s - loss: 11.9940\n",
      "Epoch 00045: val_loss improved from 11.59549 to 11.47955, saving model to models/mlp-sgd-momentum-02-lr-05/training__45__11.999053\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 11.9991 - val_loss: 11.4795\n",
      "Epoch 46/100\n",
      "664/679 [============================>.] - ETA: 0s - loss: 11.8576\n",
      "Epoch 00046: val_loss improved from 11.47955 to 11.35824, saving model to models/mlp-sgd-momentum-02-lr-05/training__46__11.856671\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 11.8567 - val_loss: 11.3582\n",
      "Epoch 47/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 11.7772\n",
      "Epoch 00047: val_loss improved from 11.35824 to 11.24330, saving model to models/mlp-sgd-momentum-02-lr-05/training__47__11.777792\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 11.7778 - val_loss: 11.2433\n",
      "Epoch 48/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 11.6365\n",
      "Epoch 00048: val_loss improved from 11.24330 to 11.12869, saving model to models/mlp-sgd-momentum-02-lr-05/training__48__11.638000\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 11.6380 - val_loss: 11.1287\n",
      "Epoch 49/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 11.5422\n",
      "Epoch 00049: val_loss improved from 11.12869 to 11.02245, saving model to models/mlp-sgd-momentum-02-lr-05/training__49__11.541812\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 11.5418 - val_loss: 11.0225\n",
      "Epoch 50/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 11.4451\n",
      "Epoch 00050: val_loss improved from 11.02245 to 10.91682, saving model to models/mlp-sgd-momentum-02-lr-05/training__50__11.445574\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 11.4456 - val_loss: 10.9168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 11.3183\n",
      "Epoch 00051: val_loss improved from 10.91682 to 10.81172, saving model to models/mlp-sgd-momentum-02-lr-05/training__51__11.316898\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 11.3169 - val_loss: 10.8117\n",
      "Epoch 52/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 11.2182\n",
      "Epoch 00052: val_loss improved from 10.81172 to 10.71284, saving model to models/mlp-sgd-momentum-02-lr-05/training__52__11.218355\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 11.2184 - val_loss: 10.7128\n",
      "Epoch 53/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 11.0544\n",
      "Epoch 00053: val_loss improved from 10.71284 to 10.61118, saving model to models/mlp-sgd-momentum-02-lr-05/training__53__11.057749\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 11.0577 - val_loss: 10.6112\n",
      "Epoch 54/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 10.9763\n",
      "Epoch 00054: val_loss improved from 10.61118 to 10.51300, saving model to models/mlp-sgd-momentum-02-lr-05/training__54__10.976814\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.9768 - val_loss: 10.5130\n",
      "Epoch 55/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 10.8903\n",
      "Epoch 00055: val_loss improved from 10.51300 to 10.42418, saving model to models/mlp-sgd-momentum-02-lr-05/training__55__10.887444\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.8874 - val_loss: 10.4242\n",
      "Epoch 56/100\n",
      "674/679 [============================>.] - ETA: 0s - loss: 10.8021\n",
      "Epoch 00056: val_loss improved from 10.42418 to 10.33652, saving model to models/mlp-sgd-momentum-02-lr-05/training__56__10.803066\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.8031 - val_loss: 10.3365\n",
      "Epoch 57/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 10.7348\n",
      "Epoch 00057: val_loss improved from 10.33652 to 10.24696, saving model to models/mlp-sgd-momentum-02-lr-05/training__57__10.739022\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.7390 - val_loss: 10.2470\n",
      "Epoch 58/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 10.5848\n",
      "Epoch 00058: val_loss improved from 10.24696 to 10.15100, saving model to models/mlp-sgd-momentum-02-lr-05/training__58__10.585447\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.5854 - val_loss: 10.1510\n",
      "Epoch 59/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 10.5469\n",
      "Epoch 00059: val_loss improved from 10.15100 to 10.06824, saving model to models/mlp-sgd-momentum-02-lr-05/training__59__10.549910\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.5499 - val_loss: 10.0682\n",
      "Epoch 60/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 10.4439\n",
      "Epoch 00060: val_loss improved from 10.06824 to 9.98412, saving model to models/mlp-sgd-momentum-02-lr-05/training__60__10.444387\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.4444 - val_loss: 9.9841\n",
      "Epoch 61/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 10.3636\n",
      "Epoch 00061: val_loss improved from 9.98412 to 9.90270, saving model to models/mlp-sgd-momentum-02-lr-05/training__61__10.361332\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.3613 - val_loss: 9.9027\n",
      "Epoch 62/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 10.2860\n",
      "Epoch 00062: val_loss improved from 9.90270 to 9.82772, saving model to models/mlp-sgd-momentum-02-lr-05/training__62__10.286208\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.2862 - val_loss: 9.8277\n",
      "Epoch 63/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 10.2166\n",
      "Epoch 00063: val_loss improved from 9.82772 to 9.74227, saving model to models/mlp-sgd-momentum-02-lr-05/training__63__10.215878\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.2159 - val_loss: 9.7423\n",
      "Epoch 64/100\n",
      "674/679 [============================>.] - ETA: 0s - loss: 10.1694\n",
      "Epoch 00064: val_loss improved from 9.74227 to 9.66547, saving model to models/mlp-sgd-momentum-02-lr-05/training__64__10.173778\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.1738 - val_loss: 9.6655\n",
      "Epoch 65/100\n",
      "664/679 [============================>.] - ETA: 0s - loss: 10.0690\n",
      "Epoch 00065: val_loss improved from 9.66547 to 9.58945, saving model to models/mlp-sgd-momentum-02-lr-05/training__65__10.070189\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 10.0702 - val_loss: 9.5894\n",
      "Epoch 66/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 9.9754\n",
      "Epoch 00066: val_loss improved from 9.58945 to 9.51662, saving model to models/mlp-sgd-momentum-02-lr-05/training__66__9.977060\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.9771 - val_loss: 9.5166\n",
      "Epoch 67/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 9.8701\n",
      "Epoch 00067: val_loss improved from 9.51662 to 9.44944, saving model to models/mlp-sgd-momentum-02-lr-05/training__67__9.870875\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.8709 - val_loss: 9.4494\n",
      "Epoch 68/100\n",
      "676/679 [============================>.] - ETA: 0s - loss: 9.8744\n",
      "Epoch 00068: val_loss improved from 9.44944 to 9.37484, saving model to models/mlp-sgd-momentum-02-lr-05/training__68__9.874992\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.8750 - val_loss: 9.3748\n",
      "Epoch 69/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 9.7418\n",
      "Epoch 00069: val_loss improved from 9.37484 to 9.30812, saving model to models/mlp-sgd-momentum-02-lr-05/training__69__9.745835\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.7458 - val_loss: 9.3081\n",
      "Epoch 70/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 9.6894\n",
      "Epoch 00070: val_loss improved from 9.30812 to 9.23735, saving model to models/mlp-sgd-momentum-02-lr-05/training__70__9.689431\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.6894 - val_loss: 9.2374\n",
      "Epoch 71/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 9.6020\n",
      "Epoch 00071: val_loss improved from 9.23735 to 9.17143, saving model to models/mlp-sgd-momentum-02-lr-05/training__71__9.600160\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.6002 - val_loss: 9.1714\n",
      "Epoch 72/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 9.5600\n",
      "Epoch 00072: val_loss improved from 9.17143 to 9.10300, saving model to models/mlp-sgd-momentum-02-lr-05/training__72__9.560631\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.5606 - val_loss: 9.1030\n",
      "Epoch 73/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 9.4919\n",
      "Epoch 00073: val_loss improved from 9.10300 to 9.04368, saving model to models/mlp-sgd-momentum-02-lr-05/training__73__9.492446\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.4924 - val_loss: 9.0437\n",
      "Epoch 74/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 9.3965\n",
      "Epoch 00074: val_loss improved from 9.04368 to 8.97659, saving model to models/mlp-sgd-momentum-02-lr-05/training__74__9.397640\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.3976 - val_loss: 8.9766\n",
      "Epoch 75/100\n",
      "664/679 [============================>.] - ETA: 0s - loss: 9.3583\n",
      "Epoch 00075: val_loss improved from 8.97659 to 8.91551, saving model to models/mlp-sgd-momentum-02-lr-05/training__75__9.354243\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.3542 - val_loss: 8.9155\n",
      "Epoch 76/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 9.2836\n",
      "Epoch 00076: val_loss improved from 8.91551 to 8.85173, saving model to models/mlp-sgd-momentum-02-lr-05/training__76__9.283730\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.2837 - val_loss: 8.8517\n",
      "Epoch 77/100\n",
      "666/679 [============================>.] - ETA: 0s - loss: 9.2253\n",
      "Epoch 00077: val_loss improved from 8.85173 to 8.79558, saving model to models/mlp-sgd-momentum-02-lr-05/training__77__9.228824\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.2288 - val_loss: 8.7956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 9.1499\n",
      "Epoch 00078: val_loss improved from 8.79558 to 8.73311, saving model to models/mlp-sgd-momentum-02-lr-05/training__78__9.142691\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.1427 - val_loss: 8.7331\n",
      "Epoch 79/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 9.1251\n",
      "Epoch 00079: val_loss improved from 8.73311 to 8.67837, saving model to models/mlp-sgd-momentum-02-lr-05/training__79__9.123446\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.1234 - val_loss: 8.6784\n",
      "Epoch 80/100\n",
      "674/679 [============================>.] - ETA: 0s - loss: 9.0632\n",
      "Epoch 00080: val_loss improved from 8.67837 to 8.61795, saving model to models/mlp-sgd-momentum-02-lr-05/training__80__9.061583\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 9.0616 - val_loss: 8.6179\n",
      "Epoch 81/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 8.9687\n",
      "Epoch 00081: val_loss improved from 8.61795 to 8.55716, saving model to models/mlp-sgd-momentum-02-lr-05/training__81__8.969835\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 8.9698 - val_loss: 8.5572\n",
      "Epoch 82/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 8.9376\n",
      "Epoch 00082: val_loss improved from 8.55716 to 8.50550, saving model to models/mlp-sgd-momentum-02-lr-05/training__82__8.938889\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 8.9389 - val_loss: 8.5055\n",
      "Epoch 83/100\n",
      "674/679 [============================>.] - ETA: 0s - loss: 8.8495\n",
      "Epoch 00083: val_loss improved from 8.50550 to 8.45163, saving model to models/mlp-sgd-momentum-02-lr-05/training__83__8.852610\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 8.8526 - val_loss: 8.4516\n",
      "Epoch 84/100\n",
      "661/679 [============================>.] - ETA: 0s - loss: 8.8158\n",
      "Epoch 00084: val_loss improved from 8.45163 to 8.39984, saving model to models/mlp-sgd-momentum-02-lr-05/training__84__8.815477\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 8.8155 - val_loss: 8.3998\n",
      "Epoch 85/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 8.7549\n",
      "Epoch 00085: val_loss improved from 8.39984 to 8.34580, saving model to models/mlp-sgd-momentum-02-lr-05/training__85__8.753859\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 8.7539 - val_loss: 8.3458\n",
      "Epoch 86/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 8.6855\n",
      "Epoch 00086: val_loss improved from 8.34580 to 8.29102, saving model to models/mlp-sgd-momentum-02-lr-05/training__86__8.686557\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 8.6866 - val_loss: 8.2910\n",
      "Epoch 87/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 8.6793\n",
      "Epoch 00087: val_loss improved from 8.29102 to 8.24324, saving model to models/mlp-sgd-momentum-02-lr-05/training__87__8.676663\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 8.6767 - val_loss: 8.2432\n",
      "Epoch 88/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 8.6279\n",
      "Epoch 00088: val_loss improved from 8.24324 to 8.18828, saving model to models/mlp-sgd-momentum-02-lr-05/training__88__8.630454\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 8.6305 - val_loss: 8.1883\n",
      "Epoch 89/100\n",
      "676/679 [============================>.] - ETA: 0s - loss: 8.5618\n",
      "Epoch 00089: val_loss improved from 8.18828 to 8.14433, saving model to models/mlp-sgd-momentum-02-lr-05/training__89__8.562494\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 8.5625 - val_loss: 8.1443\n",
      "Epoch 90/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 8.5170\n",
      "Epoch 00090: val_loss improved from 8.14433 to 8.09071, saving model to models/mlp-sgd-momentum-02-lr-05/training__90__8.515629\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 8.5156 - val_loss: 8.0907\n",
      "Epoch 91/100\n",
      "661/679 [============================>.] - ETA: 0s - loss: 8.4400\n",
      "Epoch 00091: val_loss improved from 8.09071 to 8.04274, saving model to models/mlp-sgd-momentum-02-lr-05/training__91__8.447557\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 8.4476 - val_loss: 8.0427\n",
      "Epoch 92/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 8.3930\n",
      "Epoch 00092: val_loss improved from 8.04274 to 8.00408, saving model to models/mlp-sgd-momentum-02-lr-05/training__92__8.394612\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 8.3946 - val_loss: 8.0041\n",
      "Epoch 93/100\n",
      "666/679 [============================>.] - ETA: 0s - loss: 8.3380\n",
      "Epoch 00093: val_loss improved from 8.00408 to 7.95057, saving model to models/mlp-sgd-momentum-02-lr-05/training__93__8.341333\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 8.3413 - val_loss: 7.9506\n",
      "Epoch 94/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 8.3230\n",
      "Epoch 00094: val_loss improved from 7.95057 to 7.90822, saving model to models/mlp-sgd-momentum-02-lr-05/training__94__8.324627\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 8.3246 - val_loss: 7.9082\n",
      "Epoch 95/100\n",
      "676/679 [============================>.] - ETA: 0s - loss: 8.2602\n",
      "Epoch 00095: val_loss improved from 7.90822 to 7.85456, saving model to models/mlp-sgd-momentum-02-lr-05/training__95__8.260381\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 8.2604 - val_loss: 7.8546\n",
      "Epoch 96/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 8.2026\n",
      "Epoch 00096: val_loss improved from 7.85456 to 7.81077, saving model to models/mlp-sgd-momentum-02-lr-05/training__96__8.199511\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 8.1995 - val_loss: 7.8108\n",
      "Epoch 97/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 8.1874\n",
      "Epoch 00097: val_loss improved from 7.81077 to 7.76673, saving model to models/mlp-sgd-momentum-02-lr-05/training__97__8.192303\\cp.ckpt\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 8.1923 - val_loss: 7.7667\n",
      "Epoch 98/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 8.1298\n",
      "Epoch 00098: val_loss improved from 7.76673 to 7.72770, saving model to models/mlp-sgd-momentum-02-lr-05/training__98__8.130054\\cp.ckpt\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 8.1301 - val_loss: 7.7277\n",
      "Epoch 99/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 8.0693\n",
      "Epoch 00099: val_loss improved from 7.72770 to 7.68225, saving model to models/mlp-sgd-momentum-02-lr-05/training__99__8.070414\\cp.ckpt\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 8.0704 - val_loss: 7.6822\n",
      "Epoch 100/100\n",
      "674/679 [============================>.] - ETA: 0s - loss: 8.0529\n",
      "Epoch 00100: val_loss improved from 7.68225 to 7.64137, saving model to models/mlp-sgd-momentum-02-lr-05/training__100__8.053912\\cp.ckpt\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 8.0539 - val_loss: 7.6414\n"
     ]
    }
   ],
   "source": [
    "logdir = \"logs/mlp-sgd-momentum-02-lr-05\"\n",
    "checkpoint_path = \"models/mlp-sgd-momentum-02-lr-05/training__{epoch:02d}__{loss:.6f}/cp.ckpt\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_best_only=True,\n",
    "                                                 verbose=1)\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=4,monitor='loss')\n",
    "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='loss', factor=0.5, patience=2, verbose=1, mode='auto',\n",
    "    min_delta=0.0001, cooldown=0, min_lr=0\n",
    ")\n",
    "with tf.device('/device:GPU:0'):\n",
    "    model.fit(\n",
    "        train_dataset, \n",
    "        epochs=100, \n",
    "        validation_data=val_dataset,\n",
    "        callbacks=[tensorboard_callback, cp_callback,early_stop_callback,reduce_lr_callback],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "878065.8309542743\n"
     ]
    }
   ],
   "source": [
    "res = model.predict(val_dataset)\n",
    "average = get_average(val_dataset,res,details=False)\n",
    "print(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1432), started 0:23:29 ago. (Use '!kill 1432' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3ca2528abcce7300\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3ca2528abcce7300\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP `Adagrad` lr `0.05`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adagrad(\n",
    "                learning_rate=0.05,\n",
    "                ),\n",
    "    loss='MSLE',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Layer dense_64 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "  2/679 [..............................] - ETA: 8:05 - loss: 178.2756WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0050s vs `on_train_batch_end` time: 1.4259s). Check your callbacks.\n",
      "672/679 [============================>.] - ETA: 0s - loss: 109.5081\n",
      "Epoch 00001: val_loss improved from inf to 92.01550, saving model to models/mlp-adagrad-lr-05/training__01__109.350563\\cp.ckpt\n",
      "679/679 [==============================] - 4s 6ms/step - loss: 109.3506 - val_loss: 92.0155\n",
      "Epoch 2/100\n",
      "674/679 [============================>.] - ETA: 0s - loss: 87.6707\n",
      "Epoch 00002: val_loss improved from 92.01550 to 82.03084, saving model to models/mlp-adagrad-lr-05/training__02__87.640053\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 87.6401 - val_loss: 82.0308\n",
      "Epoch 3/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 79.7184\n",
      "Epoch 00003: val_loss improved from 82.03084 to 76.15842, saving model to models/mlp-adagrad-lr-05/training__03__79.706169\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 79.7062 - val_loss: 76.1584\n",
      "Epoch 4/100\n",
      "666/679 [============================>.] - ETA: 0s - loss: 74.7272\n",
      "Epoch 00004: val_loss improved from 76.15842 to 71.78577, saving model to models/mlp-adagrad-lr-05/training__04__74.696915\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 74.6969 - val_loss: 71.7858\n",
      "Epoch 5/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 71.1046\n",
      "Epoch 00005: val_loss improved from 71.78577 to 68.70427, saving model to models/mlp-adagrad-lr-05/training__05__71.104568\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 71.1046 - val_loss: 68.7043\n",
      "Epoch 6/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 68.3353\n",
      "Epoch 00006: val_loss improved from 68.70427 to 66.34905, saving model to models/mlp-adagrad-lr-05/training__06__68.334122\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 68.3341 - val_loss: 66.3491\n",
      "Epoch 7/100\n",
      "666/679 [============================>.] - ETA: 0s - loss: 66.0558\n",
      "Epoch 00007: val_loss improved from 66.34905 to 64.21680, saving model to models/mlp-adagrad-lr-05/training__07__66.035957\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 66.0360 - val_loss: 64.2168\n",
      "Epoch 8/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 64.2252\n",
      "Epoch 00008: val_loss improved from 64.21680 to 62.52911, saving model to models/mlp-adagrad-lr-05/training__08__64.197853\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 64.1979 - val_loss: 62.5291\n",
      "Epoch 9/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 62.5480\n",
      "Epoch 00009: val_loss improved from 62.52911 to 60.98382, saving model to models/mlp-adagrad-lr-05/training__09__62.540634\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 62.5406 - val_loss: 60.9838\n",
      "Epoch 10/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 61.1189\n",
      "Epoch 00010: val_loss improved from 60.98382 to 59.56493, saving model to models/mlp-adagrad-lr-05/training__10__61.104382\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 61.1044 - val_loss: 59.5649\n",
      "Epoch 11/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 59.6920\n",
      "Epoch 00011: val_loss improved from 59.56493 to 58.38643, saving model to models/mlp-adagrad-lr-05/training__11__59.695755\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 59.6958 - val_loss: 58.3864\n",
      "Epoch 12/100\n",
      "660/679 [============================>.] - ETA: 0s - loss: 58.5925\n",
      "Epoch 00012: val_loss improved from 58.38643 to 57.34921, saving model to models/mlp-adagrad-lr-05/training__12__58.586643\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 58.5866 - val_loss: 57.3492\n",
      "Epoch 13/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 57.5699\n",
      "Epoch 00013: val_loss improved from 57.34921 to 56.30069, saving model to models/mlp-adagrad-lr-05/training__13__57.554096\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 57.5541 - val_loss: 56.3007\n",
      "Epoch 14/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 56.5372\n",
      "Epoch 00014: val_loss improved from 56.30069 to 55.34484, saving model to models/mlp-adagrad-lr-05/training__14__56.535622\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 56.5356 - val_loss: 55.3448\n",
      "Epoch 15/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 55.7137\n",
      "Epoch 00015: val_loss improved from 55.34484 to 54.49644, saving model to models/mlp-adagrad-lr-05/training__15__55.718796\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 55.7188 - val_loss: 54.4964\n",
      "Epoch 16/100\n",
      "663/679 [============================>.] - ETA: 0s - loss: 54.8159\n",
      "Epoch 00016: val_loss improved from 54.49644 to 53.65969, saving model to models/mlp-adagrad-lr-05/training__16__54.808311\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 54.8083 - val_loss: 53.6597\n",
      "Epoch 17/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 54.1936\n",
      "Epoch 00017: val_loss improved from 53.65969 to 52.99782, saving model to models/mlp-adagrad-lr-05/training__17__54.188351\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 54.1884 - val_loss: 52.9978\n",
      "Epoch 18/100\n",
      "676/679 [============================>.] - ETA: 0s - loss: 53.4480\n",
      "Epoch 00018: val_loss improved from 52.99782 to 52.33640, saving model to models/mlp-adagrad-lr-05/training__18__53.443207\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 53.4432 - val_loss: 52.3364\n",
      "Epoch 19/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 52.6741\n",
      "Epoch 00019: val_loss improved from 52.33640 to 51.66267, saving model to models/mlp-adagrad-lr-05/training__19__52.668106\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 52.6681 - val_loss: 51.6627\n",
      "Epoch 20/100\n",
      "662/679 [============================>.] - ETA: 0s - loss: 52.1314\n",
      "Epoch 00020: val_loss improved from 51.66267 to 51.06056, saving model to models/mlp-adagrad-lr-05/training__20__52.143181\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 52.1432 - val_loss: 51.0606\n",
      "Epoch 21/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 51.4398\n",
      "Epoch 00021: val_loss improved from 51.06056 to 50.49165, saving model to models/mlp-adagrad-lr-05/training__21__51.431870\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 51.4319 - val_loss: 50.4917\n",
      "Epoch 22/100\n",
      "674/679 [============================>.] - ETA: 0s - loss: 51.0181\n",
      "Epoch 00022: val_loss improved from 50.49165 to 49.97176, saving model to models/mlp-adagrad-lr-05/training__22__51.019398\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 51.0194 - val_loss: 49.9718\n",
      "Epoch 23/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 50.4811\n",
      "Epoch 00023: val_loss improved from 49.97176 to 49.44269, saving model to models/mlp-adagrad-lr-05/training__23__50.488239\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 50.4882 - val_loss: 49.4427\n",
      "Epoch 24/100\n",
      "666/679 [============================>.] - ETA: 0s - loss: 49.9594\n",
      "Epoch 00024: val_loss improved from 49.44269 to 48.92305, saving model to models/mlp-adagrad-lr-05/training__24__49.960892\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 49.9609 - val_loss: 48.9231\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "667/679 [============================>.] - ETA: 0s - loss: 49.4838\n",
      "Epoch 00025: val_loss improved from 48.92305 to 48.46318, saving model to models/mlp-adagrad-lr-05/training__25__49.479416\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 49.4794 - val_loss: 48.4632\n",
      "Epoch 26/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 48.9821\n",
      "Epoch 00026: val_loss improved from 48.46318 to 48.00824, saving model to models/mlp-adagrad-lr-05/training__26__48.984219\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 48.9842 - val_loss: 48.0082\n",
      "Epoch 27/100\n",
      "666/679 [============================>.] - ETA: 0s - loss: 48.5358\n",
      "Epoch 00027: val_loss improved from 48.00824 to 47.56115, saving model to models/mlp-adagrad-lr-05/training__27__48.524513\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 48.5245 - val_loss: 47.5611\n",
      "Epoch 28/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 48.0627\n",
      "Epoch 00028: val_loss improved from 47.56115 to 47.18626, saving model to models/mlp-adagrad-lr-05/training__28__48.066074\\cp.ckpt\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 48.0661 - val_loss: 47.1863\n",
      "Epoch 29/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 47.7315\n",
      "Epoch 00029: val_loss improved from 47.18626 to 46.76258, saving model to models/mlp-adagrad-lr-05/training__29__47.734489\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 47.7345 - val_loss: 46.7626\n",
      "Epoch 30/100\n",
      "676/679 [============================>.] - ETA: 0s - loss: 47.2274\n",
      "Epoch 00030: val_loss improved from 46.76258 to 46.39569, saving model to models/mlp-adagrad-lr-05/training__30__47.227482\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 47.2275 - val_loss: 46.3957\n",
      "Epoch 31/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 47.0575\n",
      "Epoch 00031: val_loss improved from 46.39569 to 46.02636, saving model to models/mlp-adagrad-lr-05/training__31__47.060162\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 47.0602 - val_loss: 46.0264\n",
      "Epoch 32/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 46.6157\n",
      "Epoch 00032: val_loss improved from 46.02636 to 45.67640, saving model to models/mlp-adagrad-lr-05/training__32__46.615238\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 46.6152 - val_loss: 45.6764\n",
      "Epoch 33/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 46.2409\n",
      "Epoch 00033: val_loss improved from 45.67640 to 45.30810, saving model to models/mlp-adagrad-lr-05/training__33__46.245014\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 46.2450 - val_loss: 45.3081\n",
      "Epoch 34/100\n",
      "660/679 [============================>.] - ETA: 0s - loss: 45.9322\n",
      "Epoch 00034: val_loss improved from 45.30810 to 44.96122, saving model to models/mlp-adagrad-lr-05/training__34__45.933342\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 45.9333 - val_loss: 44.9612\n",
      "Epoch 35/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 45.5204\n",
      "Epoch 00035: val_loss improved from 44.96122 to 44.65454, saving model to models/mlp-adagrad-lr-05/training__35__45.522919\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 45.5229 - val_loss: 44.6545\n",
      "Epoch 36/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 45.2920\n",
      "Epoch 00036: val_loss improved from 44.65454 to 44.34326, saving model to models/mlp-adagrad-lr-05/training__36__45.291992\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 45.2920 - val_loss: 44.3433\n",
      "Epoch 37/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 44.8769\n",
      "Epoch 00037: val_loss improved from 44.34326 to 44.05484, saving model to models/mlp-adagrad-lr-05/training__37__44.877300\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 44.8773 - val_loss: 44.0548\n",
      "Epoch 38/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 44.6652\n",
      "Epoch 00038: val_loss improved from 44.05484 to 43.75223, saving model to models/mlp-adagrad-lr-05/training__38__44.666317\\cp.ckpt\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 44.6663 - val_loss: 43.7522\n",
      "Epoch 39/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 44.4183\n",
      "Epoch 00039: val_loss improved from 43.75223 to 43.44234, saving model to models/mlp-adagrad-lr-05/training__39__44.427139\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 44.4271 - val_loss: 43.4423\n",
      "Epoch 40/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 44.1111\n",
      "Epoch 00040: val_loss improved from 43.44234 to 43.18546, saving model to models/mlp-adagrad-lr-05/training__40__44.111065\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 44.1111 - val_loss: 43.1855\n",
      "Epoch 41/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 43.7910\n",
      "Epoch 00041: val_loss improved from 43.18546 to 42.94904, saving model to models/mlp-adagrad-lr-05/training__41__43.797173\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 43.7972 - val_loss: 42.9490\n",
      "Epoch 42/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 43.5950\n",
      "Epoch 00042: val_loss improved from 42.94904 to 42.66820, saving model to models/mlp-adagrad-lr-05/training__42__43.589779\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 43.5898 - val_loss: 42.6682\n",
      "Epoch 43/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 43.3587\n",
      "Epoch 00043: val_loss improved from 42.66820 to 42.40353, saving model to models/mlp-adagrad-lr-05/training__43__43.363716\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 43.3637 - val_loss: 42.4035\n",
      "Epoch 44/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 43.0583\n",
      "Epoch 00044: val_loss improved from 42.40353 to 42.14507, saving model to models/mlp-adagrad-lr-05/training__44__43.063030\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 43.0630 - val_loss: 42.1451\n",
      "Epoch 45/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 42.7810\n",
      "Epoch 00045: val_loss improved from 42.14507 to 41.91456, saving model to models/mlp-adagrad-lr-05/training__45__42.777855\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 42.7779 - val_loss: 41.9146\n",
      "Epoch 46/100\n",
      "661/679 [============================>.] - ETA: 0s - loss: 42.4471\n",
      "Epoch 00046: val_loss improved from 41.91456 to 41.67946, saving model to models/mlp-adagrad-lr-05/training__46__42.459198\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 42.4592 - val_loss: 41.6795\n",
      "Epoch 47/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 42.2390\n",
      "Epoch 00047: val_loss improved from 41.67946 to 41.44810, saving model to models/mlp-adagrad-lr-05/training__47__42.239044\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 42.2390 - val_loss: 41.4481\n",
      "Epoch 48/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 42.0998\n",
      "Epoch 00048: val_loss improved from 41.44810 to 41.24713, saving model to models/mlp-adagrad-lr-05/training__48__42.099831\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 42.0998 - val_loss: 41.2471\n",
      "Epoch 49/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 41.8740\n",
      "Epoch 00049: val_loss improved from 41.24713 to 40.99785, saving model to models/mlp-adagrad-lr-05/training__49__41.868198\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 41.8682 - val_loss: 40.9978\n",
      "Epoch 50/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 41.6286\n",
      "Epoch 00050: val_loss improved from 40.99785 to 40.79423, saving model to models/mlp-adagrad-lr-05/training__50__41.628895\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 41.6289 - val_loss: 40.7942\n",
      "Epoch 51/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 41.4492\n",
      "Epoch 00051: val_loss improved from 40.79423 to 40.60493, saving model to models/mlp-adagrad-lr-05/training__51__41.454254\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 41.4543 - val_loss: 40.6049\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "677/679 [============================>.] - ETA: 0s - loss: 41.2269\n",
      "Epoch 00052: val_loss improved from 40.60493 to 40.37058, saving model to models/mlp-adagrad-lr-05/training__52__41.226791\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 41.2268 - val_loss: 40.3706\n",
      "Epoch 53/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 40.9516\n",
      "Epoch 00053: val_loss improved from 40.37058 to 40.17867, saving model to models/mlp-adagrad-lr-05/training__53__40.960197\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 40.9602 - val_loss: 40.1787\n",
      "Epoch 54/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 40.8119\n",
      "Epoch 00054: val_loss improved from 40.17867 to 39.98686, saving model to models/mlp-adagrad-lr-05/training__54__40.814877\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 40.8149 - val_loss: 39.9869\n",
      "Epoch 55/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 40.6186\n",
      "Epoch 00055: val_loss improved from 39.98686 to 39.79539, saving model to models/mlp-adagrad-lr-05/training__55__40.614933\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 40.6149 - val_loss: 39.7954\n",
      "Epoch 56/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 40.4265\n",
      "Epoch 00056: val_loss improved from 39.79539 to 39.63244, saving model to models/mlp-adagrad-lr-05/training__56__40.426174\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 40.4262 - val_loss: 39.6324\n",
      "Epoch 57/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 40.1746\n",
      "Epoch 00057: val_loss improved from 39.63244 to 39.42976, saving model to models/mlp-adagrad-lr-05/training__57__40.180317\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 40.1803 - val_loss: 39.4298\n",
      "Epoch 58/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 40.0797\n",
      "Epoch 00058: val_loss improved from 39.42976 to 39.26191, saving model to models/mlp-adagrad-lr-05/training__58__40.074203\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 40.0742 - val_loss: 39.2619\n",
      "Epoch 59/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 39.8811\n",
      "Epoch 00059: val_loss improved from 39.26191 to 39.05561, saving model to models/mlp-adagrad-lr-05/training__59__39.886032\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 39.8860 - val_loss: 39.0556\n",
      "Epoch 60/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 39.6723\n",
      "Epoch 00060: val_loss improved from 39.05561 to 38.91451, saving model to models/mlp-adagrad-lr-05/training__60__39.682716\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 39.6827 - val_loss: 38.9145\n",
      "Epoch 61/100\n",
      "664/679 [============================>.] - ETA: 0s - loss: 39.5877\n",
      "Epoch 00061: val_loss improved from 38.91451 to 38.71318, saving model to models/mlp-adagrad-lr-05/training__61__39.570385\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 39.5704 - val_loss: 38.7132\n",
      "Epoch 62/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 39.3003\n",
      "Epoch 00062: val_loss improved from 38.71318 to 38.55938, saving model to models/mlp-adagrad-lr-05/training__62__39.298923\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 39.2989 - val_loss: 38.5594\n",
      "Epoch 63/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 39.2111\n",
      "Epoch 00063: val_loss improved from 38.55938 to 38.37741, saving model to models/mlp-adagrad-lr-05/training__63__39.202744\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 39.2027 - val_loss: 38.3774\n",
      "Epoch 64/100\n",
      "674/679 [============================>.] - ETA: 0s - loss: 38.9976\n",
      "Epoch 00064: val_loss improved from 38.37741 to 38.21617, saving model to models/mlp-adagrad-lr-05/training__64__38.999668\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 38.9997 - val_loss: 38.2162\n",
      "Epoch 65/100\n",
      "665/679 [============================>.] - ETA: 0s - loss: 38.8641\n",
      "Epoch 00065: val_loss improved from 38.21617 to 38.07773, saving model to models/mlp-adagrad-lr-05/training__65__38.861202\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 38.8612 - val_loss: 38.0777\n",
      "Epoch 66/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 38.6758\n",
      "Epoch 00066: val_loss improved from 38.07773 to 37.91219, saving model to models/mlp-adagrad-lr-05/training__66__38.678177\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 38.6782 - val_loss: 37.9122\n",
      "Epoch 67/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 38.4809\n",
      "Epoch 00067: val_loss improved from 37.91219 to 37.73841, saving model to models/mlp-adagrad-lr-05/training__67__38.480927\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 38.4809 - val_loss: 37.7384\n",
      "Epoch 68/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 38.3765\n",
      "Epoch 00068: val_loss improved from 37.73841 to 37.58549, saving model to models/mlp-adagrad-lr-05/training__68__38.379353\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 38.3794 - val_loss: 37.5855\n",
      "Epoch 69/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 38.1861\n",
      "Epoch 00069: val_loss improved from 37.58549 to 37.44225, saving model to models/mlp-adagrad-lr-05/training__69__38.190178\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 38.1902 - val_loss: 37.4422\n",
      "Epoch 70/100\n",
      "674/679 [============================>.] - ETA: 0s - loss: 38.1005\n",
      "Epoch 00070: val_loss improved from 37.44225 to 37.29859, saving model to models/mlp-adagrad-lr-05/training__70__38.099731\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 38.0997 - val_loss: 37.2986\n",
      "Epoch 71/100\n",
      "659/679 [============================>.] - ETA: 0s - loss: 37.9356\n",
      "Epoch 00071: val_loss improved from 37.29859 to 37.15392, saving model to models/mlp-adagrad-lr-05/training__71__37.949474\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 37.9495 - val_loss: 37.1539\n",
      "Epoch 72/100\n",
      "666/679 [============================>.] - ETA: 0s - loss: 37.7600\n",
      "Epoch 00072: val_loss improved from 37.15392 to 37.01152, saving model to models/mlp-adagrad-lr-05/training__72__37.757221\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 37.7572 - val_loss: 37.0115\n",
      "Epoch 73/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 37.7412\n",
      "Epoch 00073: val_loss improved from 37.01152 to 36.87726, saving model to models/mlp-adagrad-lr-05/training__73__37.741207\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 37.7412 - val_loss: 36.8773\n",
      "Epoch 74/100\n",
      "671/679 [============================>.] - ETA: 0s - loss: 37.4866\n",
      "Epoch 00074: val_loss improved from 36.87726 to 36.72496, saving model to models/mlp-adagrad-lr-05/training__74__37.488754\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 37.4888 - val_loss: 36.7250\n",
      "Epoch 75/100\n",
      "674/679 [============================>.] - ETA: 0s - loss: 37.3461\n",
      "Epoch 00075: val_loss improved from 36.72496 to 36.60061, saving model to models/mlp-adagrad-lr-05/training__75__37.346127\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 37.3461 - val_loss: 36.6006\n",
      "Epoch 76/100\n",
      "670/679 [============================>.] - ETA: 0s - loss: 37.2076\n",
      "Epoch 00076: val_loss improved from 36.60061 to 36.46940, saving model to models/mlp-adagrad-lr-05/training__76__37.215679\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 37.2157 - val_loss: 36.4694\n",
      "Epoch 77/100\n",
      "674/679 [============================>.] - ETA: 0s - loss: 37.1212\n",
      "Epoch 00077: val_loss improved from 36.46940 to 36.34300, saving model to models/mlp-adagrad-lr-05/training__77__37.125355\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 37.1254 - val_loss: 36.3430\n",
      "Epoch 78/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 36.9771\n",
      "Epoch 00078: val_loss improved from 36.34300 to 36.21326, saving model to models/mlp-adagrad-lr-05/training__78__36.969719\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 36.9697 - val_loss: 36.2133\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "677/679 [============================>.] - ETA: 0s - loss: 36.8112\n",
      "Epoch 00079: val_loss improved from 36.21326 to 36.08418, saving model to models/mlp-adagrad-lr-05/training__79__36.814285\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 36.8143 - val_loss: 36.0842\n",
      "Epoch 80/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 36.7290\n",
      "Epoch 00080: val_loss improved from 36.08418 to 35.94577, saving model to models/mlp-adagrad-lr-05/training__80__36.728970\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 36.7290 - val_loss: 35.9458\n",
      "Epoch 81/100\n",
      "664/679 [============================>.] - ETA: 0s - loss: 36.6102\n",
      "Epoch 00081: val_loss improved from 35.94577 to 35.83304, saving model to models/mlp-adagrad-lr-05/training__81__36.618481\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 36.6185 - val_loss: 35.8330\n",
      "Epoch 82/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 36.3963\n",
      "Epoch 00082: val_loss improved from 35.83304 to 35.71941, saving model to models/mlp-adagrad-lr-05/training__82__36.399155\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 36.3992 - val_loss: 35.7194\n",
      "Epoch 83/100\n",
      "676/679 [============================>.] - ETA: 0s - loss: 36.3569\n",
      "Epoch 00083: val_loss improved from 35.71941 to 35.59559, saving model to models/mlp-adagrad-lr-05/training__83__36.359825\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 36.3598 - val_loss: 35.5956\n",
      "Epoch 84/100\n",
      "664/679 [============================>.] - ETA: 0s - loss: 36.2190\n",
      "Epoch 00084: val_loss improved from 35.59559 to 35.45034, saving model to models/mlp-adagrad-lr-05/training__84__36.217995\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 36.2180 - val_loss: 35.4503\n",
      "Epoch 85/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 36.0931\n",
      "Epoch 00085: val_loss improved from 35.45034 to 35.33151, saving model to models/mlp-adagrad-lr-05/training__85__36.092434\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 36.0924 - val_loss: 35.3315\n",
      "Epoch 86/100\n",
      "674/679 [============================>.] - ETA: 0s - loss: 36.0149\n",
      "Epoch 00086: val_loss improved from 35.33151 to 35.23021, saving model to models/mlp-adagrad-lr-05/training__86__36.015362\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 36.0154 - val_loss: 35.2302\n",
      "Epoch 87/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 35.8172\n",
      "Epoch 00087: val_loss improved from 35.23021 to 35.13849, saving model to models/mlp-adagrad-lr-05/training__87__35.818233\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 35.8182 - val_loss: 35.1385\n",
      "Epoch 88/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 35.7560\n",
      "Epoch 00088: val_loss improved from 35.13849 to 35.00820, saving model to models/mlp-adagrad-lr-05/training__88__35.760185\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 35.7602 - val_loss: 35.0082\n",
      "Epoch 89/100\n",
      "673/679 [============================>.] - ETA: 0s - loss: 35.7145\n",
      "Epoch 00089: val_loss improved from 35.00820 to 34.90288, saving model to models/mlp-adagrad-lr-05/training__89__35.723820\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 35.7238 - val_loss: 34.9029\n",
      "Epoch 90/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 35.4895\n",
      "Epoch 00090: val_loss improved from 34.90288 to 34.79920, saving model to models/mlp-adagrad-lr-05/training__90__35.493603\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 35.4936 - val_loss: 34.7992\n",
      "Epoch 91/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 35.4547\n",
      "Epoch 00091: val_loss improved from 34.79920 to 34.76391, saving model to models/mlp-adagrad-lr-05/training__91__35.457306\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 35.4573 - val_loss: 34.7639\n",
      "Epoch 92/100\n",
      "676/679 [============================>.] - ETA: 0s - loss: 35.4256\n",
      "Epoch 00092: val_loss improved from 34.76391 to 34.62981, saving model to models/mlp-adagrad-lr-05/training__92__35.426994\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 35.4270 - val_loss: 34.6298\n",
      "Epoch 93/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 35.2863\n",
      "Epoch 00093: val_loss improved from 34.62981 to 34.52119, saving model to models/mlp-adagrad-lr-05/training__93__35.282681\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 35.2827 - val_loss: 34.5212\n",
      "Epoch 94/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 35.1106\n",
      "Epoch 00094: val_loss improved from 34.52119 to 34.42319, saving model to models/mlp-adagrad-lr-05/training__94__35.110634\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 35.1106 - val_loss: 34.4232\n",
      "Epoch 95/100\n",
      "676/679 [============================>.] - ETA: 0s - loss: 35.0019\n",
      "Epoch 00095: val_loss improved from 34.42319 to 34.28665, saving model to models/mlp-adagrad-lr-05/training__95__35.002346\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 35.0023 - val_loss: 34.2867\n",
      "Epoch 96/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 34.8711\n",
      "Epoch 00096: val_loss improved from 34.28665 to 34.17149, saving model to models/mlp-adagrad-lr-05/training__96__34.873886\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 34.8739 - val_loss: 34.1715\n",
      "Epoch 97/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 34.8792\n",
      "Epoch 00097: val_loss improved from 34.17149 to 34.07304, saving model to models/mlp-adagrad-lr-05/training__97__34.879017\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 34.8790 - val_loss: 34.0730\n",
      "Epoch 98/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 34.6865\n",
      "Epoch 00098: val_loss improved from 34.07304 to 33.97411, saving model to models/mlp-adagrad-lr-05/training__98__34.681808\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 34.6818 - val_loss: 33.9741\n",
      "Epoch 99/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 34.6657\n",
      "Epoch 00099: val_loss improved from 33.97411 to 33.86834, saving model to models/mlp-adagrad-lr-05/training__99__34.666370\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 34.6664 - val_loss: 33.8683\n",
      "Epoch 100/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 34.5009\n",
      "Epoch 00100: val_loss improved from 33.86834 to 33.75924, saving model to models/mlp-adagrad-lr-05/training__100__34.506203\\cp.ckpt\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 34.5062 - val_loss: 33.7592\n"
     ]
    }
   ],
   "source": [
    "logdir = \"logs/mlp-adagrad-lr-05\"\n",
    "checkpoint_path = \"models/mlp-adagrad-lr-05/training__{epoch:02d}__{loss:.6f}/cp.ckpt\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_best_only=True,\n",
    "                                                 verbose=1)\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=4,monitor='loss')\n",
    "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='loss', factor=0.5, patience=2, verbose=1, mode='auto',\n",
    "    min_delta=0.0001, cooldown=0, min_lr=0\n",
    ")\n",
    "with tf.device('/device:GPU:0'):\n",
    "    model.fit(\n",
    "        train_dataset, \n",
    "        epochs=100, \n",
    "        validation_data=val_dataset,\n",
    "        callbacks=[tensorboard_callback, cp_callback,early_stop_callback,reduce_lr_callback],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "932003.8182737273\n"
     ]
    }
   ],
   "source": [
    "res = model.predict(val_dataset)\n",
    "average = get_average(val_dataset,res,details=False)\n",
    "print(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1432), started 0:29:48 ago. (Use '!kill 1432' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ffd4678644a8673e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ffd4678644a8673e\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP `RMSprop` lr `05` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(\n",
    "                learning_rate=0.05,\n",
    "                ),\n",
    "    loss='MSLE',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Layer dense_72 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "  2/679 [..............................] - ETA: 9:28 - loss: 167.0509WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0050s vs `on_train_batch_end` time: 1.6722s). Check your callbacks.\n",
      "678/679 [============================>.] - ETA: 0s - loss: 39.0470\n",
      "Epoch 00001: val_loss improved from inf to 14.73200, saving model to models/mlp-rmsprop-lr-05/training__01__39.013176\\cp.ckpt\n",
      "679/679 [==============================] - 5s 7ms/step - loss: 39.0132 - val_loss: 14.7320\n",
      "Epoch 2/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 10.1340\n",
      "Epoch 00002: val_loss improved from 14.73200 to 5.94058, saving model to models/mlp-rmsprop-lr-05/training__02__10.114051\\cp.ckpt\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 10.1141 - val_loss: 5.9406\n",
      "Epoch 3/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 4.4700\n",
      "Epoch 00003: val_loss improved from 5.94058 to 2.70902, saving model to models/mlp-rmsprop-lr-05/training__03__4.469977\\cp.ckpt\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 4.4700 - val_loss: 2.7090\n",
      "Epoch 4/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 2.0894\n",
      "Epoch 00004: val_loss improved from 2.70902 to 1.19141, saving model to models/mlp-rmsprop-lr-05/training__04__2.075997\\cp.ckpt\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 2.0760 - val_loss: 1.1914\n",
      "Epoch 5/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 0.9551\n",
      "Epoch 00005: val_loss improved from 1.19141 to 0.49322, saving model to models/mlp-rmsprop-lr-05/training__05__0.951279\\cp.ckpt\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 0.9513 - val_loss: 0.4932\n",
      "Epoch 6/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 0.4465\n",
      "Epoch 00006: val_loss improved from 0.49322 to 0.19724, saving model to models/mlp-rmsprop-lr-05/training__06__0.446488\\cp.ckpt\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 0.4465 - val_loss: 0.1972\n",
      "Epoch 7/100\n",
      "676/679 [============================>.] - ETA: 0s - loss: 0.2283\n",
      "Epoch 00007: val_loss improved from 0.19724 to 0.09457, saving model to models/mlp-rmsprop-lr-05/training__07__0.228343\\cp.ckpt\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 0.2283 - val_loss: 0.0946\n",
      "Epoch 8/100\n",
      "679/679 [==============================] - ETA: 0s - loss: 0.1702\n",
      "Epoch 00008: val_loss improved from 0.09457 to 0.07525, saving model to models/mlp-rmsprop-lr-05/training__08__0.170211\\cp.ckpt\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 0.1702 - val_loss: 0.0753\n",
      "Epoch 9/100\n",
      "676/679 [============================>.] - ETA: 0s - loss: 0.1550\n",
      "Epoch 00009: val_loss improved from 0.07525 to 0.07303, saving model to models/mlp-rmsprop-lr-05/training__09__0.155009\\cp.ckpt\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 0.1550 - val_loss: 0.0730\n",
      "Epoch 10/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 0.1543\n",
      "Epoch 00010: val_loss did not improve from 0.07303\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 0.1543 - val_loss: 0.0803\n",
      "Epoch 11/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 0.1525\n",
      "Epoch 00011: val_loss did not improve from 0.07303\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1526 - val_loss: 0.0739\n",
      "Epoch 12/100\n",
      "674/679 [============================>.] - ETA: 0s - loss: 0.1509\n",
      "Epoch 00012: val_loss did not improve from 0.07303\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1509 - val_loss: 0.0762\n",
      "Epoch 13/100\n",
      "668/679 [============================>.] - ETA: 0s - loss: 0.1548\n",
      "Epoch 00013: val_loss did not improve from 0.07303\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1552 - val_loss: 0.0791\n",
      "Epoch 14/100\n",
      "674/679 [============================>.] - ETA: 0s - loss: 0.1498\n",
      "Epoch 00014: val_loss did not improve from 0.07303\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1498 - val_loss: 0.0748\n",
      "Epoch 15/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 0.1461\n",
      "Epoch 00015: val_loss improved from 0.07303 to 0.07289, saving model to models/mlp-rmsprop-lr-05/training__15__0.146143\\cp.ckpt\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 0.1461 - val_loss: 0.0729\n",
      "Epoch 16/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 0.1477\n",
      "Epoch 00016: val_loss improved from 0.07289 to 0.07227, saving model to models/mlp-rmsprop-lr-05/training__16__0.147435\\cp.ckpt\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 0.1474 - val_loss: 0.0723\n",
      "Epoch 17/100\n",
      "677/679 [============================>.] - ETA: 0s - loss: 0.1495\n",
      "Epoch 00017: val_loss did not improve from 0.07227\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 0.1496 - val_loss: 0.0747\n",
      "Epoch 18/100\n",
      "671/679 [============================>.] - ETA: 0s - loss: 0.1433\n",
      "Epoch 00018: val_loss did not improve from 0.07227\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1432 - val_loss: 0.0754\n",
      "Epoch 19/100\n",
      "671/679 [============================>.] - ETA: 0s - loss: 0.1438\n",
      "Epoch 00019: val_loss did not improve from 0.07227\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1435 - val_loss: 0.0778\n",
      "Epoch 20/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 0.1452\n",
      "Epoch 00020: val_loss did not improve from 0.07227\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.012500000186264515.\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1447 - val_loss: 0.0781\n",
      "Epoch 21/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 0.1505\n",
      "Epoch 00021: val_loss did not improve from 0.07227\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1506 - val_loss: 0.0754\n",
      "Epoch 22/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 0.1429\n",
      "Epoch 00022: val_loss did not improve from 0.07227\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1428 - val_loss: 0.0786\n",
      "Epoch 23/100\n",
      "675/679 [============================>.] - ETA: 0s - loss: 0.1412\n",
      "Epoch 00023: val_loss did not improve from 0.07227\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 0.1412 - val_loss: 0.0759\n",
      "Epoch 24/100\n",
      "669/679 [============================>.] - ETA: 0s - loss: 0.1394\n",
      "Epoch 00024: val_loss did not improve from 0.07227\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 0.1393 - val_loss: 0.0795\n",
      "Epoch 25/100\n",
      "667/679 [============================>.] - ETA: 0s - loss: 0.1421\n",
      "Epoch 00025: val_loss did not improve from 0.07227\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1420 - val_loss: 0.0788\n",
      "Epoch 26/100\n",
      "676/679 [============================>.] - ETA: 0s - loss: 0.1416\n",
      "Epoch 00026: val_loss did not improve from 0.07227\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0062500000931322575.\n",
      "679/679 [==============================] - 3s 5ms/step - loss: 0.1416 - val_loss: 0.0773\n",
      "Epoch 27/100\n",
      "678/679 [============================>.] - ETA: 0s - loss: 0.1455\n",
      "Epoch 00027: val_loss did not improve from 0.07227\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1456 - val_loss: 0.0757\n",
      "Epoch 28/100\n",
      "672/679 [============================>.] - ETA: 0s - loss: 0.1419\n",
      "Epoch 00028: val_loss did not improve from 0.07227\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0031250000465661287.\n",
      "679/679 [==============================] - 3s 4ms/step - loss: 0.1421 - val_loss: 0.0778\n"
     ]
    }
   ],
   "source": [
    "logdir = \"logs/mlp-rmsprop-lr-05\"\n",
    "checkpoint_path = \"models/mlp-rmsprop-lr-05/training__{epoch:02d}__{loss:.6f}/cp.ckpt\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_best_only=True,\n",
    "                                                 verbose=1)\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=4,monitor='loss')\n",
    "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='loss', factor=0.5, patience=2, verbose=1, mode='auto',\n",
    "    min_delta=0.0001, cooldown=0, min_lr=0\n",
    ")\n",
    "with tf.device('/device:GPU:0'):\n",
    "    model.fit(\n",
    "        train_dataset, \n",
    "        epochs=100, \n",
    "        validation_data=val_dataset,\n",
    "        callbacks=[tensorboard_callback, cp_callback,early_stop_callback,reduce_lr_callback],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325792.9884683682\n"
     ]
    }
   ],
   "source": [
    "res = model.predict(val_dataset)\n",
    "average = get_average(val_dataset,res,details=False)\n",
    "print(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1432), started 0:33:32 ago. (Use '!kill 1432' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-cafbaa80aabb3f17\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-cafbaa80aabb3f17\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
